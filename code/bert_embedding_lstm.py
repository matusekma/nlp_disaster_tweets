# -*- coding: utf-8 -*-
"""BERT_embedding_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R9jDF35tjMXtCmSrmqtJnXae5ne36_A2
"""

from torch.utils.data import DataLoader
from torch.utils.data.dataset import random_split
from sklearn.model_selection import train_test_split
from textblob import TextBlob
from bs4 import BeautifulSoup
import string
import re
import nltk
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from transformers import BertTokenizer, BertModel, BertForMaskedLM
import random
import numpy as np
from tqdm import tqdm_notebook as tqdm
import time
import logging
# logging.basicConfig(level=logging.INFO)
import os
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pd.set_option('max_colwidth', 400)
print(device)

logger = logging.getLogger('mylogger')
logger.setLevel(logging.DEBUG)
timestamp = time.strftime("%Y.%m.%d_%H.%M.%S", time.localtime())
fh = logging.FileHandler('log_model.txt')
fh.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')
fh.setFormatter(formatter)
ch.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


seed_everything()

# Applying a first round of text cleaning techniques

tokenizer = nltk.tokenize.TweetTokenizer(
    strip_handles=True, reduce_len=True)

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


def clean_text_no_smiley(text):
    text = BeautifulSoup(text, 'lxml').get_text()
    eyes = "[8:=;]"
    nose = "['`\-]?"
    text = re.sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*", " ", text)

    text = re.sub("/", " / ", text)
    text = re.sub('@(\w+)', '', text)

    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', " ", text)
    text = re.sub('#{eyes}#{nose}p+', " ", text)
    text = re.sub('#{eyes}#{nose}\(+|\)+#{nose}#{eyes}', " ", text)
    text = re.sub('#{eyes}#{nose}[\/|l*]', " ", text)
    text = re.sub('<3', " ", text)
    # numbers
    text = re.sub('[-+]?[.\d]*[\d]+[:,.\d]*', " ", text)

    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = text.lower()
    # text = re.sub('\[.*?\]', '', text)
    # text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(
        string.punctuation.replace("'", "")), ' ', text)
    text = re.sub('\n', ' ', text)
    text = ''.join(filter(lambda x: x in string.printable, text))
    # Single character removal
    text = re.sub(r"\s+[a-zA-Z]\s+", ' ', text)

    # text = re.sub('\w*\d\w*', '', text)

    return text


def text_preprocessing_no_lemmatizer(text):

    nopunc = clean_text_no_smiley(text)

    tokenized_text = tokenizer.tokenize(nopunc)

    combined_text = ' '.join(tokenized_text)
    return combined_text


try:
    train = pd.read_csv('input/preprocessed_train.csv')
    print('Training data shape: ', train.shape)
    test = pd.read_csv('input/preprocessed_test.csv')
    print('Testing data shape: ', test.shape)
except:
    train = pd.read_csv('../input/train.csv')
    print('Training data shape: ', train.shape)
    test = pd.read_csv('../input/test.csv')
    print('Testing data shape: ', test.shape)

    train['text'] = train['text'].apply(
        lambda x: text_preprocessing_no_lemmatizer(x))
    test['text'] = test['text'].apply(
        lambda x: text_preprocessing_no_lemmatizer(x))

    train.drop(["keyword", "location"], axis=1, inplace=True)
    test.drop(["keyword", "location"], axis=1, inplace=True)

    train.to_csv('../input/preprocessed_train.csv')
    test.to_csv('../input/preprocessed_test.csv')


train[['text']].head(5)

# custom dataset


class BertDataset(Dataset):
    def __init__(self, input_ids, segment_ids, labels=None):
        self.input_ids = input_ids
        self.segment_ids = segment_ids

    def __len__(self):
        return (len(self.input_ids))

    def __getitem__(self, i):
        return (self.input_ids[i], self.segment_ids[i])


def convert_df_to_BERT_input(sequences):
    all_segment_ids = []
    all_input_ids = []
    bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    for index, sequence in enumerate(sequences):

        marked_text = "[CLS] " + sequence + " [SEP]"

        # Split the sentence into tokens.
        tokenized_text = bertTokenizer.tokenize(marked_text)

        # Map the token strings to their vocabulary indeces.
        input_ids = bertTokenizer.convert_tokens_to_ids(tokenized_text)

        segment_ids = [1] * len(input_ids)

        if index < 1:
            logger.info("*** Example ***")
            logger.info("idx: {}".format(index))
            logger.info("tokens: {}".format(
                ' '.join(marked_text).replace('\u2581', '_')))
            logger.info("input_ids: {}".format(' '.join(map(str, input_ids))))
            logger.info("segment_ids: {}".format(len(segment_ids)))

        all_input_ids.append(input_ids)
        all_segment_ids.append(segment_ids)

    return BertDataset(all_input_ids, all_segment_ids)


bert_input = convert_df_to_BERT_input(train["text"])

bert_model = BertModel.from_pretrained(
    'bert-base-uncased', output_hidden_states=True)
# Put the model in "evaluation" mode, meaning feed-forward operation.
bert_model.eval()

data = DataLoader(bert_input)
bert_embeddings = []
with torch.no_grad():
    for index, (input_ids, segment_ids) in enumerate(data):
        tokens_tensor = torch.tensor([input_ids])
        segments_tensors = torch.tensor([segment_ids])
        _, _, hidden_states = bert_model(tokens_tensor, segments_tensors)
        # hidden states length is 13 (1 embedding + 12 hidden layers)
        hidden_layers = hidden_states[1:]

        token_vecs = hidden_layers[-1][0]

        # Calculate the average of all token vectors.
        sentence_embedding = torch.mean(token_vecs, dim=0)
        bert_embeddings.append(sentence_embedding)
        if(index % 10 == 0):
            print(index)

print(len(bert_embeddings))


target = train["target"]
train_data, validation_data, train_target, validation_target = train_test_split(
    bert_embeddings, target, test_size=0.2, random_state=1000)
test_data = test["text"]

# custom dataset


class TwitterDataset(Dataset):
    def __init__(self, texts, labels=None, transforms=None):
        self.X = texts
        if labels is not None:
            self.y = torch.tensor(np.asarray(labels), dtype=torch.long)
        else:
            self.y = None
        self.transforms = transforms

    def __len__(self):
        return (len(self.X))

    def __getitem__(self, i):
        data = self.X[i]
        if self.y is not None:
            return (data, self.y[i])
        else:
            return data


train_data = TwitterDataset(train_data, train_target)
validation_data = TwitterDataset(validation_data, validation_target)
#test_data = TwitterDataset(test_data)
print(train_data[0])
print(validation_data[0])

# hyperparameters
seq_length = 768
hidden_dim = 256
learning_rate = 0.05  # 1e-5
num_epochs = 10
batch_size = 8
patience = 2
num_layers = 2
bidirectional = False


class TwitterClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, num_layers):
        super().__init__()

        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, bidirectional=bidirectional)

        self.dropout = nn.Dropout(0.8)
        #hidden_dim * num_layers * (1+self.config.bidirectional), 2
        self.linear = nn.Linear(hidden_dim, 2)

    def forward(self, sentence):
        # batch size is 1

        sentence = sentence[0]
        lstm_out, _ = self.lstm(sentence.view(1, 1, -1))
        linear_in = self.dropout(lstm_out.view(1, -1))
        linear = self.linear(linear_in.view(1, -1))
        return linear.view(1, 2)


model = TwitterClassifier(seq_length, hidden_dim, num_layers).to(device)
model


def prec_rec_F1(labels, preds):
    # true positives
    tp = 0
    # false negatives
    fn = 0
    for label, pred in zip(labels, preds):
        if label == 1:
            if pred == 1:
                tp += 1
            else:
                fn += 1

    pospreds = sum(preds)
    precision = tp / pospreds
    recall = tp / (fn + tp)
    try:
        f1 = 2 * precision * recall / (precision + recall)
    except ZeroDivisionError:
        return (precision, recall, 0.0)
    return (precision, recall, f1)


def train_func(sub_train_):

    # Train the model
    train_loss = 0
    train_acc = 0
    labels = []
    preds = []
    # dataloaders
    data = DataLoader(sub_train_, shuffle=True)
    for i, (text, cls) in enumerate(data):
        optimizer.zero_grad()
        text, cls = text.to(device), cls.to(device)
        output = model(text)
        loss = criterion(output, cls)
        train_loss += loss.item()
        loss.backward()
        optimizer.step()
        pred = output.argmax(1)
        train_acc += (pred == cls).sum().item()
        labels.append(cls.item())
        preds.append(pred.item())

    # Adjust the learning rate
    scheduler.step()

    return train_loss / len(sub_train_), train_acc / len(sub_train_), prec_rec_F1(labels, preds)


def validate_func(data_):
    loss = 0
    acc = 0
    labels = []
    preds = []
    data = DataLoader(data_)
    for text, cls in data:
        text, cls = text.to(device), cls.to(device)
        with torch.no_grad():
            output = model(text)
            l = criterion(output, cls)
            loss += l.item()
            pred = output.argmax(1)
            acc += (pred == cls).sum().item()
            labels.append(cls.item())
            preds.append(pred.item())

    return loss / len(data_), acc / len(data_), prec_rec_F1(labels, preds)


min_valid_loss = float('inf')

criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)

sub_train_, sub_valid_ = train_data, validation_data
all_train_loss = []
all_valid_loss = []
all_train_acc = []
all_valid_acc = []

best_F1 = 0.
early_stop = 0
for epoch in range(num_epochs):

    start_time = time.time()

    train_loss, train_acc, (train_precision, train_recall,
                            train_F1) = train_func(sub_train_)
    valid_loss, valid_acc, (valid_precision, valid_recall,
                            valid_F1) = validate_func(sub_valid_)

    all_train_loss.append(train_loss)
    all_train_acc.append(train_acc)
    all_valid_loss.append(valid_loss)
    all_valid_acc.append(valid_acc)

    if best_F1 < valid_F1:
        early_stop = 0
        best_f1 = valid_F1
    else:
        early_stop += 1

    if early_stop >= patience:
        break

    secs = int(time.time() - start_time)
    mins = secs / 60
    secs = secs % 60

    print('Epoch: %d' % (epoch + 1),
          " | time in %d minutes, %d seconds" % (mins, secs))
    print(f'\t\tF1 score: {train_F1:.2f} (train)\t\tF1 score: {valid_F1:.2f} (valid)\n\t\tPrecision: {train_precision:.2f} (train)\t\tPrecision: {valid_precision:.2f} (valid)\n\t\tRecall: {train_recall:.2f} (train)\t\tRecall: {valid_recall:.2f} (valid)')

plt.plot(all_train_loss, label='train')
plt.plot(all_valid_loss, label='validation')
plt.legend()
plt.plot(all_train_acc, label='train')
plt.plot(all_valid_acc, label='validation')
plt.legend()

bert_input = convert_df_to_BERT_input(test_data)

data = DataLoader(bert_input)
bert_embeddings = []
with torch.no_grad():
    for index, (input_ids, segment_ids) in enumerate(data):
        tokens_tensor = torch.tensor([input_ids])
        segments_tensors = torch.tensor([segment_ids])
        _, _, hidden_states = bert_model(tokens_tensor, segments_tensors)
        # hidden states length is 13 (1 embedding + 12 hidden layers)
        hidden_layers = hidden_states[1:]

        token_vecs = hidden_layers[-1][0]

        # Calculate the average of all token vectors.
        sentence_embedding = torch.mean(token_vecs, dim=0)
        bert_embeddings.append(sentence_embedding)


def predict_func(test_data_):
    predictions = []
    data = DataLoader(test_data_)
    for text in data:
        text = text.to(device)
        with torch.no_grad():
            output = model(text)
            predictions.append(output.argmax(1))

    return predictions


predictions = predict_func(bert_embeddings)
print(len(predictions))


def submission(submission_file_path, submission_data):
    sample_submission = pd.read_csv(submission_file_path)
    sample_submission["target"] = [tensor.cpu().numpy()[0]
                                   for tensor in submission_data]
    print(sample_submission["target"])
    sample_submission.to_csv("submission.csv", index=False)


submission_file_path = "input/sample_submission.csv"
submission(submission_file_path, predictions)
