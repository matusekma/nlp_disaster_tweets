{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "BERT_embedding_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6430fbf34eb64ff0ae3c96bcabcfeaf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aae3d889d340496099843ff28b59bc98",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a68349f99c51430395ce5d6857f98f4d",
              "IPY_MODEL_bbe1682fe434442e953b94eb177d033f"
            ]
          }
        },
        "aae3d889d340496099843ff28b59bc98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a68349f99c51430395ce5d6857f98f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba775cf5a22b4b4aa0f433e0fcb1b47f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e4263a5710c4f68b42056a58a42f83f"
          }
        },
        "bbe1682fe434442e953b94eb177d033f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3297d8e0c6a14b439aab712eeb425a95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 805kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab3c2196f8454b3386d41101fb5f0bac"
          }
        },
        "ba775cf5a22b4b4aa0f433e0fcb1b47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e4263a5710c4f68b42056a58a42f83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3297d8e0c6a14b439aab712eeb425a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab3c2196f8454b3386d41101fb5f0bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV6N7kXBd_Y3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "d2c3f2cf-ad22-425f-88c5-9f2776787412"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 12.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 54.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 62.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=0c21c097e169661d22cf84946f89056b28d42c5aeb4782cd62d4330752c762ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FUPVzY-dxl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "2c5918dd-a5ab-422f-c3df-77d424f04c62"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import time\n",
        "import logging\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pd.set_option('max_colwidth', 400)\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GITi3zmJdxl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger('mylogger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
        "fh = logging.FileHandler('log_model.txt')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
        "fh.setFormatter(formatter)\n",
        "ch.setFormatter(formatter)\n",
        "logger.addHandler(fh)\n",
        "logger.addHandler(ch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0FOBB6LdxmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_everything()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22mOYGEWdxmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "485c5e40-c997-42ac-8e2a-7b4f4e8206f6"
      },
      "source": [
        "# Applying a first round of text cleaning techniques\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, pos_tag\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "\n",
        "tokenizer = nltk.tokenize.TweetTokenizer(\n",
        "        strip_handles=True, reduce_len=True)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def clean_text_no_smiley(text):\n",
        "    text = BeautifulSoup(text, 'lxml').get_text()\n",
        "    eyes = \"[8:=;]\"\n",
        "    nose = \"['`\\-]?\"\n",
        "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"/\", \" / \", text)\n",
        "    text = re.sub('@(\\w+)', '', text)\n",
        "\n",
        "    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', \" \", text)\n",
        "    text = re.sub('#{eyes}#{nose}p+', \" \", text)\n",
        "    text = re.sub('#{eyes}#{nose}\\(+|\\)+#{nose}#{eyes}', \" \", text)\n",
        "    text = re.sub('#{eyes}#{nose}[\\/|l*]', \" \", text)\n",
        "    text = re.sub('<3', \" \", text)\n",
        "    # numbers\n",
        "    text = re.sub('[-+]?[.\\d]*[\\d]+[:,.\\d]*', \" \", text)\n",
        "\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    # text = re.sub('\\[.*?\\]', '', text)\n",
        "    # text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(\n",
        "        string.punctuation.replace(\"'\", \"\")), ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
        "    # Single character removal\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "\n",
        "    # text = re.sub('\\w*\\d\\w*', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_preprocessing_no_lemmatizer(text):\n",
        "\n",
        "    nopunc = clean_text_no_smiley(text)\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "\n",
        "    combined_text = ' '.join(tokenized_text)\n",
        "    return combined_text\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RNknW6QdxmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c8d16610-cd74-40c4-8c95-0d084c4728bf"
      },
      "source": [
        "try:\n",
        "    train = pd.read_csv('input/preprocessed_train.csv')\n",
        "    print('Training data shape: ', train.shape)\n",
        "    test = pd.read_csv('input/preprocessed_test.csv')\n",
        "    print('Testing data shape: ', test.shape)\n",
        "except:\n",
        "    train = pd.read_csv('../input/train.csv')\n",
        "    print('Training data shape: ', train.shape)\n",
        "    test = pd.read_csv('../input/test.csv')\n",
        "    print('Testing data shape: ', test.shape)\n",
        "\n",
        "    train['text'] = train['text'].apply(\n",
        "        lambda x: text_preprocessing_no_lemmatizer(x))\n",
        "    test['text'] = test['text'].apply(\n",
        "        lambda x: text_preprocessing_no_lemmatizer(x))\n",
        "\n",
        "    train.drop([\"keyword\", \"location\"], axis=1, inplace=True)\n",
        "    test.drop([\"keyword\", \"location\"], axis=1, inplace=True)\n",
        "\n",
        "    train.to_csv('../input/preprocessed_train.csv')\n",
        "    test.to_csv('../input/preprocessed_test.csv')\n",
        "\n",
        "\n",
        "train[['text']].head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (7613, 4)\n",
            "Testing data shape:  (3263, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>our deeds are the reason of this earthquake may allah forgive us all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>all residents asked to ' shelter in place ' are being notified by officers no other evacuation or shelter in place orders are expected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>people receive wildfires evacuation orders in california</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>just got sent this photo from ruby alaska as smoke from wildfires pours into school</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                     text\n",
              "0                                                                    our deeds are the reason of this earthquake may allah forgive us all\n",
              "1                                                                                                   forest fire near la ronge sask canada\n",
              "2  all residents asked to ' shelter in place ' are being notified by officers no other evacuation or shelter in place orders are expected\n",
              "3                                                                                people receive wildfires evacuation orders in california\n",
              "4                                                     just got sent this photo from ruby alaska as smoke from wildfires pours into school"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjaNY9s1dxmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom dataset\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, input_ids, segment_ids, labels=None):\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.input_ids))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return (self.input_ids[i], self.segment_ids[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrmFNv5XdxmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_df_to_BERT_input(sequences):\n",
        "    all_segment_ids = []\n",
        "    all_input_ids = []\n",
        "    bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    for index, sequence in enumerate(sequences):\n",
        "\n",
        "        marked_text = \"[CLS] \" + sequence + \" [SEP]\"\n",
        "\n",
        "        # Split the sentence into tokens.\n",
        "        tokenized_text = bertTokenizer.tokenize(marked_text)\n",
        "\n",
        "        # Map the token strings to their vocabulary indeces.\n",
        "        input_ids = bertTokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segment_ids = [1] * len(input_ids)\n",
        "        \n",
        "        if index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"idx: {}\".format(index))\n",
        "            logger.info(\"tokens: {}\".format(\n",
        "                ' '.join(marked_text).replace('\\u2581', '_')))\n",
        "            logger.info(\"input_ids: {}\".format(' '.join(map(str, input_ids))))\n",
        "            logger.info(\"segment_ids: {}\".format(len(segment_ids)))\n",
        "        \n",
        "        all_input_ids.append(input_ids)\n",
        "        all_segment_ids.append(segment_ids)\n",
        "        \n",
        "    return BertDataset(all_input_ids, all_segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm6hVWCqdxmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "6430fbf34eb64ff0ae3c96bcabcfeaf4",
            "aae3d889d340496099843ff28b59bc98",
            "a68349f99c51430395ce5d6857f98f4d",
            "bbe1682fe434442e953b94eb177d033f",
            "ba775cf5a22b4b4aa0f433e0fcb1b47f",
            "3e4263a5710c4f68b42056a58a42f83f",
            "3297d8e0c6a14b439aab712eeb425a95",
            "ab3c2196f8454b3386d41101fb5f0bac"
          ]
        },
        "outputId": "3b270d19-149e-41d3-ec7f-ed74f187a664"
      },
      "source": [
        "bert_input = convert_df_to_BERT_input(train[\"text\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6430fbf34eb64ff0ae3c96bcabcfeaf4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-03-27 14:52:19,075][INFO] ## *** Example ***\n",
            "[2020-03-27 14:52:19,076][INFO] ## idx: 0\n",
            "[2020-03-27 14:52:19,077][INFO] ## tokens: [ C L S ]   o u r   d e e d s   a r e   t h e   r e a s o n   o f   t h i s   e a r t h q u a k e   m a y   a l l a h   f o r g i v e   u s   a l l   [ S E P ]\n",
            "[2020-03-27 14:52:19,078][INFO] ## input_ids: 101 2256 15616 2024 1996 3114 1997 2023 8372 2089 16455 9641 2149 2035 102\n",
            "[2020-03-27 14:52:19,080][INFO] ## segment_ids: 15\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27eB98CXdxmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "bert_model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMQS8mrydxmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataLoader(bert_input)\n",
        "bert_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for index, (input_ids, segment_ids) in enumerate(data):\n",
        "        tokens_tensor = torch.tensor([input_ids])\n",
        "        segments_tensors = torch.tensor([segment_ids])\n",
        "        _, _, hidden_states = bert_model(tokens_tensor, segments_tensors)\n",
        "        # hidden states length is 13 (1 embedding + 12 hidden layers)\n",
        "        hidden_layers = hidden_states[1:]\n",
        "        \n",
        "        token_vecs = hidden_layers[-1][0]\n",
        "\n",
        "        # Calculate the average of all token vectors.\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        bert_embeddings.append(sentence_embedding)\n",
        "        if(index % 10 == 0):\n",
        "          print(index)                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSDxzwJ7dxmr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e34ba5a-8c51-48f4-e3e1-6a4a73cba32e"
      },
      "source": [
        "print(len(bert_embeddings))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMgdUr6Edxmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = train[\"target\"]\n",
        "train_data, validation_data, train_target, validation_target = train_test_split(\n",
        "   bert_embeddings, target, test_size=0.2, random_state=1000)\n",
        "test_data = test[\"text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci9M5Z2zdxmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0989f860-2183-439d-fd6d-84647c719300"
      },
      "source": [
        "# custom dataset\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, texts, labels=None, transforms=None):\n",
        "        self.X = texts\n",
        "        if labels is not None:\n",
        "            self.y = torch.tensor(np.asarray(labels), dtype=torch.long)\n",
        "        else:\n",
        "            self.y = None\n",
        "        self.transforms = transforms\n",
        "         \n",
        "    def __len__(self):\n",
        "        return (len(self.X))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        data = self.X[i]\n",
        "        if self.y is not None:\n",
        "            return (data, self.y[i])\n",
        "        else:\n",
        "            return data\n",
        "        \n",
        "train_data = TwitterDataset(train_data, train_target)\n",
        "validation_data = TwitterDataset(validation_data, validation_target)\n",
        "#test_data = TwitterDataset(test_data)\n",
        "print(train_data[0])\n",
        "print(validation_data[0])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([ 1.7276e-01, -1.4717e-01,  2.1108e-01,  8.0237e-02, -4.1827e-02,\n",
            "        -1.7690e-01,  5.3020e-01,  3.3603e-01, -5.2205e-02, -4.4691e-01,\n",
            "         2.0859e-01, -1.7992e-01,  4.3141e-01,  3.9740e-01, -2.8991e-01,\n",
            "         5.6983e-02,  1.9456e-01, -8.3470e-02, -7.6050e-02,  1.3379e-01,\n",
            "         8.3556e-03,  3.1709e-02, -6.7024e-02,  3.0285e-01,  1.2726e-01,\n",
            "        -6.1541e-02, -1.3746e-01,  6.1880e-02, -2.1069e-01,  3.2704e-01,\n",
            "        -2.0449e-01,  1.7793e-01,  3.8459e-02, -1.2663e-01, -9.9837e-02,\n",
            "         1.1544e-02,  2.0193e-01, -2.6348e-01, -1.4333e-01,  4.1471e-01,\n",
            "        -7.2417e-02,  1.1098e-02,  4.3211e-01,  2.6134e-01,  3.8920e-02,\n",
            "        -2.9475e-01,  3.5310e-01, -9.5596e-02,  6.8993e-02, -3.2745e-02,\n",
            "        -2.3039e-01,  3.1718e-01,  9.7187e-02, -1.0199e-01, -8.7137e-02,\n",
            "         5.1300e-01,  3.4346e-01, -6.6475e-02, -2.6083e-01,  3.0425e-01,\n",
            "         4.2535e-01,  4.1903e-02,  2.4820e-01, -3.4435e-01, -7.1811e-02,\n",
            "         3.0142e-01, -7.9299e-02,  2.8590e-01, -7.7766e-01,  3.6177e-01,\n",
            "         1.0841e-01, -1.0837e-01, -3.8241e-01,  4.3959e-02,  3.2704e-03,\n",
            "         1.4312e-02, -1.5104e-01,  5.2948e-01, -3.8597e-02, -5.6537e-01,\n",
            "        -2.4967e-01,  5.1437e-01, -5.9006e-01,  8.0873e-01,  1.6475e-01,\n",
            "        -5.7213e-02, -2.2033e-01,  1.6380e-03, -2.9770e-01,  6.5496e-01,\n",
            "        -1.4155e-01, -1.8930e-01, -2.2077e-01,  4.5716e-01,  6.7763e-02,\n",
            "        -1.0934e-01, -2.1944e-01,  2.1619e-01,  1.1378e-01,  6.5119e-01,\n",
            "         2.9489e-01, -1.9250e-01,  3.4383e-01,  4.3698e-01,  2.0071e-01,\n",
            "        -1.5362e-01, -1.3994e-02,  3.5057e-01,  4.9799e-01, -4.2124e-01,\n",
            "         1.7733e-02, -3.4974e-01, -1.6616e-01, -2.7514e-01, -4.3145e-01,\n",
            "        -9.9509e-02,  3.2431e-01, -8.3561e-01, -2.4537e-01, -3.4948e-02,\n",
            "        -2.6331e-01, -2.0002e-01, -9.3584e-03,  5.2065e-01, -1.7988e-01,\n",
            "         1.6136e-01, -4.9595e-02,  3.7071e-01, -1.8854e-01,  1.2394e-01,\n",
            "         1.8394e-01,  6.1539e-01, -1.8192e-01, -6.4427e-01, -9.9285e-02,\n",
            "         3.8784e-01,  4.9420e-01, -1.7680e-01, -4.4909e-01, -1.3773e-01,\n",
            "         1.9334e-01,  1.1791e-01,  1.7987e-01, -1.7566e-01,  1.7621e-01,\n",
            "        -1.0881e-01, -2.2969e-01, -5.6418e-01, -1.6933e-01,  9.2663e-02,\n",
            "        -3.6267e-02, -3.1584e-01, -1.0584e-02, -2.0076e-01, -3.9658e-01,\n",
            "        -9.8341e-02, -3.2491e-01, -1.1953e-02,  7.6074e-02,  4.8369e-01,\n",
            "         4.3655e-01, -4.8337e-01, -2.1660e-02,  1.2694e-01, -2.4291e-01,\n",
            "         5.4153e-01,  1.8973e-01,  3.4314e-01,  4.7264e-02, -1.8900e-01,\n",
            "        -2.2672e-01,  4.8597e-02,  6.2648e-01, -1.3165e-01, -4.9633e-02,\n",
            "        -7.5009e-02, -2.2530e-01, -9.8472e-02, -3.1614e-01, -5.1484e-02,\n",
            "        -1.3492e+00,  3.7316e-01,  1.9292e-01,  7.7124e-02,  3.5174e-01,\n",
            "        -1.5949e-01,  7.4907e-02, -2.0845e-01, -9.0543e-02, -1.0129e-01,\n",
            "         2.3624e-02, -2.5072e-01, -3.8718e-01,  1.3931e-01,  2.8480e-01,\n",
            "        -8.7622e-01, -4.4131e-01, -2.1717e-01, -5.8156e-01,  2.5787e-03,\n",
            "         5.4951e-02,  1.7213e-01,  4.4322e-01,  2.1226e-01, -3.8849e-01,\n",
            "         7.5018e-01,  3.1753e-01, -1.1559e-01, -5.6101e-03,  9.1502e-02,\n",
            "        -9.1020e-01,  1.3761e-01,  3.1161e-01, -9.9426e-04,  3.5832e-01,\n",
            "        -3.9607e-01,  1.5346e-01, -8.7936e-01, -1.0819e-01, -1.9870e-02,\n",
            "         7.2703e-03,  1.2533e-01, -5.6700e-01, -2.5181e-01,  5.9945e-02,\n",
            "         5.1489e-01, -4.8709e-01, -2.4153e-01, -3.7312e-01,  6.5260e-01,\n",
            "        -8.6174e-02, -3.2380e-01,  6.5327e-01,  2.8030e-01, -1.1124e-01,\n",
            "        -3.2808e-01, -2.6661e-02, -3.0400e-01,  1.1294e-01, -5.0753e-02,\n",
            "        -2.9710e-01, -1.0114e-01,  6.1323e-01,  1.3337e-01,  2.6974e-01,\n",
            "        -2.8310e-01,  1.1958e-01, -3.6596e-01, -1.8615e-01, -2.6163e-01,\n",
            "        -4.6606e-02, -7.5583e-01, -1.5968e-01, -6.6529e-01, -7.9924e-01,\n",
            "        -2.6923e-01, -1.5310e-01, -2.2904e-01, -2.3673e-01, -5.2247e-02,\n",
            "        -1.6980e-01, -1.2976e-01, -5.7871e-03,  1.8557e-01, -4.5723e-01,\n",
            "         4.9030e-02, -5.2970e-02,  3.3143e-01,  8.8651e-01,  1.2857e-01,\n",
            "         8.7356e-02, -1.1045e-01,  8.0319e-02,  3.5722e-01, -4.2157e-01,\n",
            "        -2.4532e-01,  2.1370e-01,  2.5717e-02,  9.9137e-02, -2.3191e-01,\n",
            "         5.4361e-01,  5.8382e-01, -4.1470e-01, -1.1409e-01,  3.0389e-01,\n",
            "        -2.8045e-01, -1.9302e-01,  1.4108e-02,  3.5414e-03, -9.0165e-02,\n",
            "        -7.7844e-02,  2.7111e-01, -1.5513e-01,  9.4541e-02,  9.0831e-02,\n",
            "        -3.1563e-01,  7.0819e-02,  1.3361e-01,  7.1145e-01, -1.4178e-01,\n",
            "         6.3739e-02,  1.3511e-01, -8.2968e-02,  3.4136e-01,  1.7285e-01,\n",
            "         2.0470e-01,  1.4875e-02, -1.5006e-01, -3.2263e+00, -4.6307e-02,\n",
            "         2.7030e-03, -5.6833e-01,  5.7060e-01, -1.5360e-01, -2.1947e-01,\n",
            "        -1.1998e-01, -5.0369e-01, -7.0690e-03,  3.7522e-01, -1.8462e-01,\n",
            "         6.1858e-01,  3.2489e-01,  3.4031e-01, -8.8761e-02,  2.6084e-01,\n",
            "        -6.1593e-01, -4.6350e-01,  3.0431e-01, -4.3814e-02, -3.4540e-01,\n",
            "         5.5106e-01, -5.4696e-01,  3.3134e-01,  4.1858e-01, -1.8455e-01,\n",
            "         3.6815e-01, -3.0335e-01,  1.5334e-01,  6.5605e-02,  2.3652e-02,\n",
            "         3.7725e-01, -4.6759e-01,  7.9039e-02,  6.3713e-02,  1.8792e-02,\n",
            "        -1.2873e-01,  1.8367e-01, -2.4141e-01, -4.9297e-01, -2.3366e-01,\n",
            "        -3.9440e-01,  9.0558e-02,  9.2135e-01, -2.0468e-01, -2.2535e-02,\n",
            "        -3.4910e-01,  3.6035e-01,  3.7156e-01,  1.9169e-01, -1.7653e-01,\n",
            "         6.0821e-03, -2.2547e-01, -1.0254e-01, -4.7180e-01,  1.6174e-01,\n",
            "         3.3775e-01, -2.5800e-01, -4.7373e-01,  7.0638e-02,  1.6452e-01,\n",
            "        -3.3017e-01,  2.3269e-02,  1.9202e-01, -3.9014e-01, -1.1486e-01,\n",
            "        -1.4358e-01, -1.8008e-01,  2.4030e-01, -3.2346e-01,  1.8411e-01,\n",
            "        -1.5043e-01, -1.0196e+00, -7.7710e-02,  2.5288e-01,  1.5129e-01,\n",
            "        -2.2249e-01,  2.5115e-01, -6.3512e-01,  1.2004e-01, -4.5159e-01,\n",
            "        -1.4334e-01, -1.5600e-01, -7.6077e-02, -6.4685e-01,  2.9257e-01,\n",
            "        -1.1334e-01, -1.3442e-01, -5.9416e-01,  7.2011e-02,  8.6867e-02,\n",
            "         1.1439e-01,  3.0276e-01,  3.6478e-01, -1.6514e-01,  2.6894e-01,\n",
            "        -2.0799e-01,  1.8628e-01, -3.5394e-01,  2.7804e-01,  7.3801e-02,\n",
            "         2.1011e-01,  7.5946e-02, -8.6373e-02, -1.8806e-01, -4.6100e-01,\n",
            "         1.5301e-02, -3.7579e-01,  9.3417e-03,  2.2170e-01, -4.8506e-01,\n",
            "         5.0250e-01, -1.7189e-01, -1.0532e-01, -2.9125e-01,  3.4065e-01,\n",
            "         6.3326e-01, -1.5474e-01,  8.0282e-02,  1.3161e-01,  6.9486e-01,\n",
            "        -1.9268e-01, -3.3214e-01, -7.5714e-01, -5.3401e-02, -3.6244e-02,\n",
            "        -9.7777e-03, -3.3269e-01, -2.8795e-01, -4.3728e-01,  1.6054e-01,\n",
            "        -3.1792e-01, -3.4135e-01, -6.0593e-02, -2.7115e-02, -1.2673e-01,\n",
            "        -4.9064e-01,  3.7273e-01,  6.1246e-01,  1.9675e-01,  5.4138e-02,\n",
            "        -1.5496e-01, -2.4956e-01, -1.7608e-01,  3.8394e-01, -2.8316e-01,\n",
            "        -2.1635e-01,  4.6962e-02,  4.5906e-01, -2.7532e-01,  1.0173e-02,\n",
            "         3.3049e-01, -3.5814e-01,  5.4553e-02, -5.0453e-01,  5.1060e-01,\n",
            "        -1.1169e-01, -5.9874e-02, -6.3802e-02, -4.2378e-01,  4.5385e-02,\n",
            "         3.3745e-01, -6.9528e-02, -3.5804e-01, -1.5082e-01, -3.6227e-02,\n",
            "        -8.4466e-02, -2.7350e-01, -1.1833e-01, -2.2175e-01, -1.0284e-03,\n",
            "        -2.3030e-01,  6.4130e-03, -6.3024e-01,  4.6534e-01,  1.4180e-02,\n",
            "        -3.7052e-01, -4.9375e-01,  2.4825e-01,  7.5409e-02, -3.2355e-01,\n",
            "        -3.2339e-01, -3.0744e-01,  3.9821e-01,  4.3468e-01,  1.3022e-01,\n",
            "         9.1239e-03,  2.4115e-01,  2.6033e-01,  1.3326e-01,  9.6260e-02,\n",
            "        -3.4159e-01, -2.7961e-01, -3.0813e-01, -1.9446e-01,  5.0232e-01,\n",
            "         9.2940e-02,  3.2009e-01,  6.6824e-01,  4.3130e-01,  2.4268e-03,\n",
            "        -1.3415e-02,  2.4732e-01, -1.3001e-01, -3.3281e-01, -3.5551e-01,\n",
            "         4.9511e-01, -1.7509e-01, -6.3863e-02, -3.2440e-01, -5.1487e-01,\n",
            "        -5.3578e-01, -3.0143e-01,  1.0481e-01, -7.7342e-02,  4.4423e-01,\n",
            "        -1.5942e-01, -1.4160e-01,  8.9108e-02, -5.1042e-02, -3.0947e-01,\n",
            "         8.9956e-02, -1.6462e-01, -4.7835e-01,  2.3371e-01, -4.4446e-01,\n",
            "         5.2045e-01, -3.7993e-01, -1.4453e-01,  3.2905e-01, -4.1563e-01,\n",
            "         3.7903e-02, -2.2578e-01,  3.9029e-01,  3.7916e-01, -7.9184e-01,\n",
            "        -2.5590e-01,  2.9784e-01,  1.3537e-01, -1.9589e-01, -7.8098e-02,\n",
            "        -1.8442e-01, -5.4871e-01,  1.5951e-01,  4.8216e-01, -7.2622e-02,\n",
            "         4.5731e-01, -1.0274e-01,  1.3219e-01, -7.9012e-02,  7.7971e-02,\n",
            "        -2.7388e-01, -3.3240e-01,  3.2439e-01, -1.2430e-01, -7.5888e-01,\n",
            "         1.1274e-01, -3.2371e-01, -1.2117e-01, -3.4021e-01, -3.2321e-01,\n",
            "        -2.4711e-01,  1.0997e-01,  3.8392e-01,  1.3105e-01, -3.5606e-02,\n",
            "         7.1581e-01,  2.5713e-01,  3.0708e-01,  1.5238e-02,  9.5770e-02,\n",
            "        -3.0660e-02, -1.6327e-01, -5.5758e-01,  1.5086e-01, -2.9057e-01,\n",
            "        -2.6893e-01, -1.3037e-01,  1.4420e-01, -3.7611e-02,  5.0047e-01,\n",
            "        -1.4571e-01, -7.4956e-02, -4.7463e-01, -8.1524e-02,  1.8564e-01,\n",
            "         8.3486e-02, -7.6410e-02, -2.1082e-01,  2.0619e-01,  1.5533e-01,\n",
            "         1.2042e-01,  1.3285e-01,  3.9947e-01,  2.8705e-01,  1.9742e-01,\n",
            "         1.1286e-01, -1.7891e-01,  5.9144e-01, -2.3643e-01,  4.7605e-01,\n",
            "         3.8199e-02, -6.5289e-02,  1.2288e-01,  8.7527e-02,  2.6373e-01,\n",
            "        -9.8005e-02,  1.7586e-01,  6.5168e-01, -1.8912e-01, -3.7018e-01,\n",
            "         1.7348e-01,  4.5808e-01, -2.0577e-01, -6.0577e-02,  2.5864e-01,\n",
            "        -1.5514e-01, -2.9384e-01,  4.2427e-01, -1.8021e-01, -1.6462e-01,\n",
            "         5.3240e-01, -1.0634e-01,  2.8415e-01,  6.7230e-01, -4.0654e-01,\n",
            "        -1.0908e-01, -2.1715e-01,  5.7749e-01,  1.4528e-01,  3.6578e-01,\n",
            "        -3.3648e-01,  4.5324e-01, -1.2788e-01, -2.0349e-01,  4.5867e-01,\n",
            "         2.4816e-01,  3.3793e-01, -4.3216e-01, -3.5551e-02, -8.7334e-02,\n",
            "         1.8514e-01,  5.8768e-01,  9.0580e-02, -2.8814e-01, -4.1430e-01,\n",
            "         5.1455e-01,  8.9815e-01,  3.4056e-01,  9.9047e-02,  2.2470e-01,\n",
            "         3.3975e-01, -5.5584e-01,  7.2663e-01,  1.0991e-01,  3.7365e-01,\n",
            "         6.0985e-01, -9.1001e-02,  2.5337e-01,  5.2570e-02,  1.7854e-01,\n",
            "         3.6150e-01, -4.8548e-01,  1.2135e-01,  3.4633e-01,  3.7913e-01,\n",
            "        -1.6679e-01, -7.9164e-01, -2.8644e-01,  6.1929e-02,  1.2074e-01,\n",
            "         5.4124e-02,  3.4005e-01,  2.8622e-01,  4.2967e-01, -5.9881e-01,\n",
            "        -9.8061e-02, -2.2740e-01,  1.1649e-01,  1.8955e-01,  7.8282e-02,\n",
            "        -5.5115e-01, -2.9808e-01,  2.6068e-01, -3.2244e-01, -1.8001e-01,\n",
            "        -1.3602e-01,  1.1092e-01, -3.5501e-01,  8.5261e-02,  6.0702e-02,\n",
            "         2.4136e-01, -5.9622e-02,  1.4922e-01, -3.1902e-01,  4.1081e-01,\n",
            "         9.8617e-02, -1.6921e-01,  3.8307e-01, -2.0842e-04,  3.1496e-02,\n",
            "        -3.9185e-01,  1.1565e-01, -4.9630e-02,  1.1286e-01, -2.9986e-01,\n",
            "         1.9811e-01, -4.4866e-02, -7.7610e-02,  6.5986e-01,  4.5894e-01,\n",
            "        -1.1667e+00,  3.8026e-01,  3.3981e-02, -8.6897e-02, -9.5708e-02,\n",
            "        -1.5327e-01, -1.1341e-01,  4.7813e-02, -5.9250e-01, -9.3756e-02,\n",
            "         2.1091e-02,  2.0778e-01, -3.2068e-01, -9.7528e-02, -4.0679e-01,\n",
            "         6.4822e-01,  3.0394e-01,  1.2780e-02, -2.4894e-01,  4.1000e-01,\n",
            "         9.8993e-02, -3.2570e-01,  3.8840e-01,  2.7894e-01, -3.7853e-01,\n",
            "        -1.9227e-01, -2.6203e-02, -4.1603e-01,  6.7075e-01,  2.2374e-03,\n",
            "         4.6066e-01,  1.0360e-01, -6.6972e-01, -5.8252e-01, -3.1697e-01,\n",
            "         2.0884e-01,  3.2977e-01, -2.7539e-01,  4.4929e-01, -1.9759e-01,\n",
            "         3.8620e-01, -5.5377e-02,  4.2190e-01, -2.9253e-01, -2.0327e-02,\n",
            "        -3.4675e-01,  2.3070e-01, -6.5799e-01]), tensor(0))\n",
            "(tensor([-3.6460e-01, -1.2665e-01,  3.2692e-01, -1.2784e-01,  4.8182e-02,\n",
            "        -5.2955e-02,  5.6350e-02,  9.0273e-02, -1.8901e-01,  2.9665e-01,\n",
            "         1.5225e-01, -4.1699e-01, -2.3519e-02,  3.6164e-01, -2.8171e-01,\n",
            "         5.2532e-01,  1.6889e-01,  1.3319e-01, -2.8069e-01,  1.0585e-01,\n",
            "         3.5523e-01, -2.6836e-01,  1.0407e-01,  4.6771e-01,  3.9205e-01,\n",
            "         3.0752e-01,  9.1834e-02,  7.5505e-02, -4.5987e-01, -9.8525e-02,\n",
            "         5.5427e-01,  2.1426e-01,  6.5156e-02, -1.3167e-01, -1.2105e-01,\n",
            "        -3.0234e-01, -2.8299e-01, -3.7576e-01, -8.1519e-02,  2.1035e-01,\n",
            "        -5.2645e-01, -1.6410e-01, -3.8901e-01,  8.7212e-03,  1.1783e-01,\n",
            "        -3.6625e-01,  6.5116e-01, -3.2206e-01,  3.3573e-01,  2.1759e-01,\n",
            "        -3.4506e-01,  4.0492e-02, -1.5766e-01, -1.9114e-01,  1.7679e-01,\n",
            "         5.6621e-01, -1.8434e-01, -7.5420e-01, -8.3799e-02, -1.7304e-01,\n",
            "         3.8374e-01, -2.6672e-01,  2.9230e-01, -3.5496e-01,  4.4232e-02,\n",
            "         9.3248e-02, -6.6503e-02,  8.1335e-01, -6.2704e-01,  1.2265e-01,\n",
            "        -2.3813e-01,  1.3399e-01, -8.7949e-02, -6.5390e-02, -6.9080e-02,\n",
            "        -1.1591e-01, -7.8769e-02, -6.7810e-02,  1.8595e-02, -1.8731e-01,\n",
            "        -8.4186e-02,  3.4760e-01, -5.8673e-01,  4.1949e-01,  1.5508e-01,\n",
            "        -9.4197e-02, -5.1018e-02, -1.3335e-01, -2.9102e-01,  8.4060e-01,\n",
            "         4.7744e-01, -7.6790e-02, -1.0559e-02,  3.2066e-01,  1.3102e-01,\n",
            "         2.5751e-01,  1.7185e-01,  1.2072e-01, -7.6912e-03,  4.0605e-02,\n",
            "         4.1135e-01, -1.0013e-01, -8.4697e-02,  4.4282e-01, -8.5684e-02,\n",
            "        -1.8466e-01,  1.4451e-01,  3.8660e-01,  7.6092e-02, -4.6521e-02,\n",
            "         1.2177e-01, -3.5098e-01,  2.0971e-01, -2.7991e-01, -3.4032e-01,\n",
            "         2.8866e-01,  3.1037e-01, -2.5158e-02, -4.7159e-02,  1.2098e-01,\n",
            "        -2.7285e-01, -2.8515e-01, -2.2352e-01,  1.1500e+00, -1.7415e-01,\n",
            "         2.0899e-01, -1.2554e-01,  1.3869e-01, -1.0672e-02,  1.1692e-01,\n",
            "         1.7871e-01,  1.0959e-01,  7.3893e-02, -2.4601e-01, -2.1570e-01,\n",
            "         4.4928e-01, -1.3069e-01, -4.6374e-01, -5.2694e-01,  1.9373e-01,\n",
            "        -5.4767e-02, -6.3497e-02,  4.2886e-01,  1.2361e-01,  2.7499e-01,\n",
            "         2.1324e-01, -3.7637e-01,  2.2545e-01, -5.3722e-02,  4.4597e-01,\n",
            "        -4.4836e-02, -1.0401e-01, -2.3340e-01, -2.2000e-01, -2.3811e-01,\n",
            "         2.2513e-01, -1.6134e-01, -4.7976e-02, -1.9035e-01,  8.2921e-02,\n",
            "         4.0067e-01, -8.3896e-02, -3.5354e-01,  1.1735e-01,  2.1915e-01,\n",
            "         2.3030e-02, -3.3496e-01,  2.2842e-01, -3.7000e-01, -1.5623e-01,\n",
            "        -1.0970e-01, -1.9890e-01,  7.6065e-01, -4.0389e-01,  6.1080e-02,\n",
            "        -1.6869e-01, -2.5466e-01, -5.3915e-02,  1.7590e-01,  2.4082e-01,\n",
            "        -5.5729e-01,  3.5077e-01, -2.3464e-01,  4.2988e-01, -1.2248e-01,\n",
            "        -5.7292e-02, -2.7116e-01, -2.4034e-01,  2.4824e-03, -1.8195e-01,\n",
            "        -4.4223e-02, -3.9703e-01, -2.4984e-01,  2.0759e-01,  2.2082e-01,\n",
            "        -3.4411e-01,  2.5820e-01, -4.1752e-01, -2.6967e-01,  6.0125e-02,\n",
            "         6.1922e-04,  1.8612e-01,  2.8518e-01,  4.5380e-01, -3.5360e-01,\n",
            "        -1.1910e-02,  3.7639e-01, -2.2371e-01, -2.9834e-01,  5.2941e-01,\n",
            "        -9.4283e-02,  4.0624e-01,  5.6803e-02,  2.2542e-01,  1.0657e-02,\n",
            "         1.8298e-01, -1.0685e-01,  2.5101e-01,  6.2470e-01, -1.0478e-01,\n",
            "        -2.3178e-01,  2.8903e-01, -1.9512e-01,  3.9193e-01, -1.6504e-01,\n",
            "         5.8335e-01, -4.2465e-02, -3.3247e-01, -2.2195e-01,  3.2139e-01,\n",
            "         1.4419e-01, -7.2706e-02,  7.2971e-01,  4.6361e-02, -5.4892e-01,\n",
            "        -1.0625e-01, -4.2036e-01, -2.9248e-01,  2.0064e-01, -2.1758e-01,\n",
            "        -2.8678e-01,  5.9620e-01,  6.4329e-01, -8.8996e-02,  2.2751e-01,\n",
            "        -2.1042e-01, -6.0090e-02,  4.1416e-01, -2.3658e-01, -2.3019e-01,\n",
            "        -2.1552e-01, -4.6997e-01,  2.0247e-01, -4.3156e-01,  2.2743e-01,\n",
            "        -4.5735e-01, -7.0339e-02, -7.1520e-02, -2.8061e-02,  2.7612e-03,\n",
            "         2.0471e-01,  3.9511e-02,  3.3275e-01, -8.9318e-03, -6.3034e-01,\n",
            "        -3.0098e-01, -1.8192e-01,  3.9564e-01,  1.4634e-01,  1.4366e-01,\n",
            "         2.4835e-02,  1.9948e-01,  2.0979e-01,  3.3805e-01, -3.6244e-02,\n",
            "         8.0805e-02,  2.1107e-01,  1.6560e-01, -2.5076e-03, -2.8705e-01,\n",
            "         1.3351e-02,  3.4467e-01,  1.1994e-01, -1.4751e-01,  2.3077e-01,\n",
            "        -6.3324e-02, -6.1509e-02,  3.0080e-01, -3.4373e-01, -4.3048e-01,\n",
            "        -2.2106e-01,  2.6098e-01,  1.5283e-01,  1.9058e-01,  4.0254e-01,\n",
            "         1.3812e-01,  1.1522e-01,  5.8202e-01,  6.6918e-01, -3.3755e-01,\n",
            "        -1.7097e-01,  2.8946e-01, -1.5594e-01,  2.6248e-01, -1.2375e-01,\n",
            "         7.2090e-02,  4.1199e-02, -3.9140e-01, -3.3951e+00, -5.4316e-02,\n",
            "        -7.7574e-02, -2.2852e-01,  3.4184e-01, -1.1587e-01,  9.8959e-02,\n",
            "        -3.0725e-01, -8.7168e-01,  3.6531e-01,  9.6578e-02, -5.0619e-01,\n",
            "         7.0051e-01, -2.2067e-01,  3.7921e-01, -1.1801e-01,  4.4342e-02,\n",
            "        -4.5150e-01, -1.5154e-01,  4.6984e-01, -7.1391e-02, -5.5364e-01,\n",
            "        -1.6594e-01, -1.9018e-01,  3.6744e-01,  2.7976e-01, -1.3611e-01,\n",
            "        -1.6953e-01, -3.1894e-01, -2.9706e-01,  2.5853e-01, -2.3969e-01,\n",
            "        -5.2663e-02,  2.7979e-01,  1.2897e-02,  2.6569e-02,  4.3170e-02,\n",
            "        -2.9765e-01, -2.0951e-01, -1.4309e-01, -2.4467e-01, -9.1865e-01,\n",
            "         9.4441e-02, -2.9870e-02,  4.5594e-01, -4.7391e-01,  2.1654e-02,\n",
            "        -6.2822e-01, -3.4698e-02,  1.0405e-01,  7.0925e-02, -1.7156e-01,\n",
            "        -1.1428e-01,  3.1387e-01,  7.1224e-02,  5.3726e-02,  2.3591e-01,\n",
            "         7.3495e-01, -4.2760e-01, -4.4309e-01,  9.6751e-03, -2.3598e-01,\n",
            "        -7.8376e-01, -7.7364e-02, -1.0998e-01,  1.0400e-02, -2.3928e-01,\n",
            "        -4.7647e-01,  2.4012e-01, -4.6057e-02, -1.2941e-01,  2.7865e-01,\n",
            "        -4.0890e-01, -6.1104e-01, -1.1094e-01,  2.1165e-02, -1.3509e-03,\n",
            "         1.9610e-01, -5.3731e-02, -1.4763e-01, -2.6905e-01, -5.2674e-01,\n",
            "         9.2600e-02, -4.3084e-01,  1.0578e-01, -1.2454e-01, -2.7600e-01,\n",
            "         1.1428e-01, -1.5795e-01, -1.5946e-01,  2.3494e-01,  2.7033e-01,\n",
            "        -1.7115e-01,  5.3566e-01, -1.0345e-01,  9.2075e-02,  1.8800e-01,\n",
            "        -1.6461e-01,  1.6787e-01, -2.3442e-01, -1.5634e-01, -3.4682e-02,\n",
            "         5.3949e-01, -4.9165e-01, -4.2634e-02,  1.3283e-01, -3.6562e-02,\n",
            "         1.1060e-02, -1.6589e-01, -2.0663e-02,  4.9929e-01, -7.6927e-01,\n",
            "         5.7859e-01, -7.0041e-02,  7.6625e-02, -1.5191e-01,  2.4800e-01,\n",
            "         4.5136e-01, -9.3244e-02,  3.4453e-02, -2.7150e-02,  5.4108e-02,\n",
            "        -3.9502e-01, -3.2110e-01, -3.0743e-01,  9.6597e-02, -3.3274e-01,\n",
            "         1.6852e-01, -6.8736e-02, -7.8723e-02,  1.4233e-01, -1.2680e-01,\n",
            "        -3.2079e-01,  3.4568e-01,  3.4007e-01, -2.1009e-01,  6.5301e-02,\n",
            "        -3.9165e-01, -1.4411e-01,  1.3854e-01,  1.7581e-01, -4.2256e-02,\n",
            "         8.6115e-03, -2.6223e-01,  9.8529e-02,  1.7124e-01,  1.3656e-01,\n",
            "        -1.9341e-01, -6.0981e-02,  1.7674e-01, -3.8015e-01, -5.5430e-01,\n",
            "         2.3323e-01,  1.5479e-01, -9.4745e-03,  2.1408e-01,  3.5001e-02,\n",
            "        -1.5922e-01, -1.7710e-01, -1.8731e-01, -1.8392e-01,  2.4430e-02,\n",
            "         3.0537e-01, -1.2984e-01, -3.6037e-01,  7.9168e-01, -8.1844e-02,\n",
            "         1.8634e-01,  2.6887e-01,  3.7973e-01, -4.4042e-02, -6.7330e-01,\n",
            "         4.6161e-02, -2.4086e-01, -2.4371e-01,  5.5940e-01, -1.7660e-01,\n",
            "        -1.7364e-01, -3.6228e-02,  3.8996e-01, -2.7316e-02,  2.4644e-02,\n",
            "        -2.2526e-02,  3.0962e-01,  3.6717e-01,  1.6062e-01,  3.5543e-01,\n",
            "        -2.8662e-01,  5.2456e-02, -2.0260e-01, -6.8875e-01,  3.0179e-01,\n",
            "         6.5896e-02,  5.3516e-02, -2.4123e-01, -3.4593e-01,  1.5928e-01,\n",
            "        -5.8961e-01,  1.0855e-01,  3.1372e-01, -9.1077e-02, -1.7377e-01,\n",
            "         1.8712e-01,  1.1768e-01, -3.1033e-01, -2.4566e-01, -1.7860e-01,\n",
            "         1.6324e-01, -6.3531e-02,  1.3657e-01, -3.1693e-02, -2.9530e-01,\n",
            "        -5.5294e-01, -1.0000e-02,  1.6849e-01, -4.5459e-01,  2.1059e-02,\n",
            "         9.8460e-03, -2.1447e-01,  3.7319e-01, -2.6008e-01,  2.6874e-01,\n",
            "        -1.5741e-01, -1.3332e-02, -5.7522e-01, -3.3831e-01,  2.1286e-01,\n",
            "         6.9273e-02,  4.5283e-02, -8.6679e-02, -9.7188e-02, -3.5741e-01,\n",
            "        -1.5201e-02,  1.6875e-01, -1.3635e-01,  3.5245e-02,  3.4620e-01,\n",
            "        -6.4466e-01, -6.0256e-02, -1.0637e-01, -7.3605e-02,  3.9595e-01,\n",
            "         2.0399e-01, -4.5509e-01,  1.4182e-01, -5.2567e-03,  2.6968e-02,\n",
            "         2.3154e-01,  6.8019e-02,  6.1252e-01, -1.7888e-01, -5.5836e-02,\n",
            "         3.3769e-01, -2.3994e-01,  1.2375e-01, -5.0081e-01, -1.8626e-01,\n",
            "        -1.1207e-01, -4.4713e-02,  1.6792e-02, -2.8897e-01, -1.0401e-01,\n",
            "        -2.6537e-01,  2.9350e-01,  2.3168e-02,  2.2003e-01,  8.1748e-02,\n",
            "         4.7441e-01,  1.8032e-02, -3.4983e-01,  5.0377e-02, -5.3954e-01,\n",
            "         1.9615e-01, -1.4456e-01, -1.2410e-01,  1.6494e-01, -3.4126e-01,\n",
            "        -1.0139e-01,  1.7455e-01, -2.6272e-01, -1.9368e-01,  3.8205e-02,\n",
            "         1.8761e-01, -1.0953e-01,  2.7727e-01,  3.7554e-01, -2.0508e-01,\n",
            "         4.2006e-02, -1.9927e-01, -3.0149e-01,  4.1901e-01,  2.8553e-01,\n",
            "        -5.2620e-02,  1.2506e-01, -3.2990e-02,  3.1961e-01,  3.8906e-01,\n",
            "         3.1613e-01,  3.0327e-01,  6.7690e-03, -2.9939e-01,  8.8608e-02,\n",
            "        -1.5721e-01, -2.1329e-01, -3.8790e-01,  5.8701e-02, -8.6931e-02,\n",
            "        -5.0914e-01,  3.6209e-01,  4.6788e-02,  1.5864e-01, -3.8938e-01,\n",
            "         6.6508e-01,  1.7461e-01, -3.1519e-01, -1.0828e-01, -1.4323e-01,\n",
            "        -3.8331e-02,  1.7486e-01,  1.4621e-01,  1.1371e-01,  1.2514e-01,\n",
            "         9.1323e-01, -5.3002e-02,  3.3316e-02,  2.3820e-01, -3.3095e-01,\n",
            "         3.5276e-02, -1.7752e-01,  5.9675e-01, -2.0831e-01, -2.1839e-01,\n",
            "        -1.2319e-01,  4.3998e-01, -3.5006e-01,  2.8261e-02,  1.1844e-01,\n",
            "        -8.7760e-02, -2.4186e-01, -2.0890e-01,  1.0199e-01,  8.7558e-03,\n",
            "         8.0565e-02,  6.5631e-01,  5.2430e-01,  2.1633e-01,  3.5581e-01,\n",
            "         1.6086e-02,  7.7715e-01,  3.8574e-01,  9.4132e-02,  1.4171e-01,\n",
            "         3.1547e-01, -2.3422e-01, -2.2564e-01,  1.5929e-01,  2.1109e-01,\n",
            "         3.0637e-01, -3.2712e-01,  4.9255e-01, -5.8712e-02, -1.4817e-01,\n",
            "         5.1735e-01, -3.5297e-01, -6.1266e-02, -1.6768e-01,  3.5203e-01,\n",
            "        -4.9470e-01, -2.3694e-01, -3.0395e-02,  9.2837e-02, -3.6583e-01,\n",
            "        -3.5878e-01, -3.5379e-01, -2.3989e-01,  4.0477e-01,  9.0622e-02,\n",
            "        -2.6999e-01, -2.9411e-01,  2.0752e-01, -3.8198e-01, -3.3199e-01,\n",
            "        -5.5246e-01, -4.9751e-01, -9.9477e-02, -3.0565e-01, -7.6129e-02,\n",
            "         1.8717e-01, -4.1271e-03, -2.3566e-01,  1.3501e-01,  2.1252e-01,\n",
            "         3.9533e-01, -8.2245e-02, -2.4080e-02, -2.8628e-01,  4.9739e-01,\n",
            "         6.3441e-02,  2.7752e-01,  1.9056e-01, -8.5121e-02, -4.5484e-01,\n",
            "        -4.0909e-01, -1.5956e-01,  2.2968e-01,  2.3479e-01, -1.2329e-01,\n",
            "         2.6198e-01, -2.6942e-01, -7.0956e-02, -1.5455e-01,  3.9992e-01,\n",
            "        -5.0786e-01, -9.8921e-02,  1.4162e-01, -9.1420e-02,  8.2588e-02,\n",
            "        -4.9629e-02, -3.3928e-01,  2.6743e-01, -1.7967e-01, -9.7017e-02,\n",
            "         9.9508e-02,  2.8456e-01, -5.8345e-01,  2.8403e-01,  1.3935e-01,\n",
            "         1.1054e-01,  2.0165e-01, -6.5597e-03, -9.8334e-02,  8.5575e-02,\n",
            "         1.4832e-02, -4.3069e-02,  1.5001e-01, -2.5214e-01, -5.3265e-02,\n",
            "        -1.6575e-01, -5.6487e-01,  1.4167e-01,  6.0312e-01,  3.4225e-02,\n",
            "         2.6605e-01, -5.3738e-02,  1.1141e-01, -1.4255e-01, -1.3864e-01,\n",
            "        -1.7639e-01,  2.3712e-01, -5.2299e-01, -1.0381e-01, -4.5623e-02,\n",
            "        -1.1257e-01, -2.2511e-01,  6.5475e-02, -1.4536e-01, -3.4649e-01,\n",
            "        -1.4969e-01, -1.5902e-03, -1.3152e-01]), tensor(1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orfvGyPodxmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "seq_length = 768\n",
        "hidden_dim = 256\n",
        "learning_rate = 0.05 #1e-5\n",
        "num_epochs = 10\n",
        "batch_size = 8\n",
        "patience = 2\n",
        "num_layers = 2\n",
        "bidirectional = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wim9iMXHdxm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwitterClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, \n",
        "                            num_layers=num_layers, bidirectional=bidirectional)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.8)\n",
        "        #hidden_dim * num_layers * (1+self.config.bidirectional), 2\n",
        "        self.linear = nn.Linear(hidden_dim, 2)\n",
        "          \n",
        "    def forward(self, sentence):\n",
        "        # batch size is 1\n",
        "        \n",
        "        sentence = sentence[0]\n",
        "        lstm_out, _ = self.lstm(sentence.view(1, 1, -1))\n",
        "        linear_in = self.dropout(lstm_out.view(1, -1))\n",
        "        linear = self.linear(linear_in.view(1, -1))\n",
        "        return linear.view(1,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EVs6FsLdxm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7925c15c-9752-424d-bb5e-62764c9e92f3"
      },
      "source": [
        "model = TwitterClassifier(seq_length, hidden_dim, num_layers).to(device)\n",
        "model"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TwitterClassifier(\n",
              "  (lstm): LSTM(768, 256, num_layers=4)\n",
              "  (dropout): Dropout(p=0.8, inplace=False)\n",
              "  (linear): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUleb_VpdxnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prec_rec_F1(labels, preds):\n",
        "    # true positives\n",
        "    tp = 0\n",
        "    # false negatives\n",
        "    fn = 0\n",
        "    for label, pred in zip(labels, preds):\n",
        "        if label == 1:\n",
        "            if pred == 1:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "                \n",
        "    pospreds = sum(preds)\n",
        "    precision = tp / pospreds\n",
        "    recall = tp / (fn + tp)\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        return (precision, recall, 0.0)\n",
        "    return (precision, recall, f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u-3tnF0dxnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    labels = []\n",
        "    preds = []\n",
        "    # dataloaders\n",
        "    data = DataLoader(sub_train_, shuffle=True)\n",
        "    for i, (text, cls) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        text, cls = text.to(device), cls.to(device)\n",
        "        output = model(text)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = output.argmax(1)\n",
        "        train_acc += (pred == cls).sum().item()\n",
        "        labels.append(cls.item())\n",
        "        preds.append(pred.item())\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_), prec_rec_F1(labels, preds)\n",
        "\n",
        "def validate_func(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    labels = []\n",
        "    preds = []\n",
        "    data = DataLoader(data_)\n",
        "    for text, cls in data:\n",
        "        text, cls = text.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text)\n",
        "            l = criterion(output, cls)\n",
        "            loss += l.item()\n",
        "            pred = output.argmax(1)\n",
        "            acc += (pred == cls).sum().item()\n",
        "            labels.append(cls.item())\n",
        "            preds.append(pred.item())\n",
        "            \n",
        "    return loss / len(data_), acc / len(data_), prec_rec_F1(labels, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWfaidu4dxnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "3d4a72d5-63a8-4be7-d64e-5ff4bea11aa6"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "sub_train_, sub_valid_ = train_data, validation_data\n",
        "all_train_loss = []\n",
        "all_valid_loss = []\n",
        "all_train_acc = []\n",
        "all_valid_acc = []\n",
        "\n",
        "best_F1 = 0.\n",
        "early_stop = 0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, (train_precision, train_recall, train_F1) = train_func(sub_train_)\n",
        "    valid_loss, valid_acc, (valid_precision, valid_recall, valid_F1) = validate_func(sub_valid_)\n",
        "\n",
        "    all_train_loss.append(train_loss)\n",
        "    all_train_acc.append(train_acc)\n",
        "    all_valid_loss.append(valid_loss)\n",
        "    all_valid_acc.append(valid_acc)\n",
        "    \n",
        "    if best_F1 < valid_F1:\n",
        "      early_stop = 0\n",
        "      best_f1 = valid_F1\n",
        "    else:\n",
        "      early_stop += 1\n",
        "\n",
        "    if early_stop >= patience:\n",
        "      break\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\t\\tF1 score: {train_F1:.2f} (train)\\t\\tF1 score: {valid_F1:.2f} (valid)\\n\\t\\tPrecision: {train_precision:.2f} (train)\\t\\tPrecision: {valid_precision:.2f} (valid)\\n\\t\\tRecall: {train_recall:.2f} (train)\\t\\tRecall: {valid_recall:.2f} (valid)')"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-149739f184ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_F1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_F1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_valid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mall_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-178-d5940c116e5c>\u001b[0m in \u001b[0;36mvalidate_func\u001b[0;34m(data_)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_rec_F1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-177-1674dcc81266>\u001b[0m in \u001b[0;36mprec_rec_F1\u001b[0;34m(labels, preds)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpospreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpospreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OjBc1tV1QoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "eef134ef-da20-42c2-858a-8796bff87a36"
      },
      "source": [
        "plt.plot(all_train_loss, label='train')\n",
        "plt.plot(all_valid_loss, label='validation')\n",
        "plt.legend()\n",
        "plt.plot(all_train_acc, label='train')\n",
        "plt.plot(all_valid_acc, label='validation')\n",
        "plt.legend()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0ddfd7f588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaUvKinsi6St",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cfc53a2b-dc79-42e0-d91e-8e53488d86af"
      },
      "source": [
        "bert_input = convert_df_to_BERT_input(test_data)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-03-27 16:54:21,680][INFO] ## *** Example ***\n",
            "[2020-03-27 16:54:21,681][INFO] ## idx: 0\n",
            "[2020-03-27 16:54:21,682][INFO] ## tokens: [ C L S ]   j u s t   h a p p e n e d   t e r r i b l e   c a r   c r a s h   [ S E P ]\n",
            "[2020-03-27 16:54:21,683][INFO] ## input_ids: 101 2074 3047 6659 2482 5823 102\n",
            "[2020-03-27 16:54:21,687][INFO] ## segment_ids: 7\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqnJ5SGopXwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataLoader(bert_input)\n",
        "bert_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for index, (input_ids, segment_ids) in enumerate(data):\n",
        "        tokens_tensor = torch.tensor([input_ids])\n",
        "        segments_tensors = torch.tensor([segment_ids])\n",
        "        _, _, hidden_states = bert_model(tokens_tensor, segments_tensors)\n",
        "        # hidden states length is 13 (1 embedding + 12 hidden layers)\n",
        "        hidden_layers = hidden_states[1:]\n",
        "        \n",
        "        token_vecs = hidden_layers[-1][0]\n",
        "\n",
        "        # Calculate the average of all token vectors.\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        bert_embeddings.append(sentence_embedding)                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YZwE_iZo8pV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5f299ba-0e49-4ce1-b687-498cd7e95ebc"
      },
      "source": [
        "def predict_func(test_data_):\n",
        "    predictions = []\n",
        "    data = DataLoader(test_data_)\n",
        "    for text in data:\n",
        "        text = text.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text)\n",
        "            predictions.append(output.argmax(1))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "predictions = predict_func(bert_embeddings)\n",
        "print(len(predictions))\n"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2yhlG8epa2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def submission(submission_file_path,submission_data):\n",
        "    sample_submission = pd.read_csv(submission_file_path)\n",
        "    sample_submission[\"target\"] = [tensor.cpu().numpy()[0] for tensor in submission_data]\n",
        "    print(sample_submission[\"target\"])\n",
        "    sample_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDAayQKCp0CV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bbb72d6b-3d22-44b0-8b55-903154e763c8"
      },
      "source": [
        "submission_file_path = \"input/sample_submission.csv\"\n",
        "submission(submission_file_path,predictions)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "3258    0\n",
            "3259    1\n",
            "3260    1\n",
            "3261    1\n",
            "3262    1\n",
            "Name: target, Length: 3263, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycO0ptsErEka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}