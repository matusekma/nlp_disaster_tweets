{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "BERT_embedding_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV6N7kXBd_Y3",
        "colab_type": "code",
        "outputId": "898a71e1-dc6a-4f47-8778-ef5dcd149a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install wordsegment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.33)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.33)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FUPVzY-dxl3",
        "colab_type": "code",
        "outputId": "f3860171-4661-4af6-a8e5-9acc58b44668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm, trange\n",
        "import time\n",
        "import logging\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pd.set_option('max_colwidth', 400)\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GITi3zmJdxl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger('mylogger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
        "fh = logging.FileHandler('log_model.txt')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
        "fh.setFormatter(formatter)\n",
        "ch.setFormatter(formatter)\n",
        "logger.addHandler(fh)\n",
        "logger.addHandler(ch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0FOBB6LdxmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_everything()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22mOYGEWdxmE",
        "colab_type": "code",
        "outputId": "c6c6b54e-92d5-4da6-fc76-3811f81711b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Applying a first round of text cleaning techniques\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "tokenizer = nltk.tokenize.TweetTokenizer(\n",
        "        strip_handles=True, reduce_len=True)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def clean_text_no_smiley(text):\n",
        "    text = BeautifulSoup(text, 'lxml').get_text()\n",
        "    eyes = \"[8:=;]\"\n",
        "    nose = \"['`\\-]?\"\n",
        "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"/\", \" / \", text)\n",
        "    text = re.sub('@(\\w+)', '', text)\n",
        "\n",
        "    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', \" \", text)\n",
        "    text = re.sub('#{eyes}#{nose}p+', \" \", text)\n",
        "    text = re.sub('#{eyes}#{nose}\\(+|\\)+#{nose}#{eyes}', \" \", text)\n",
        "    text = re.sub('#{eyes}#{nose}[\\/|l*]', \" \", text)\n",
        "    text = re.sub('<3', \" \", text)\n",
        "    # numbers\n",
        "    text = re.sub('[-+]?[.\\d]*[\\d]+[:,.\\d]*', \" \", text)\n",
        "\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('[%s]' % re.escape(\n",
        "        string.punctuation.replace(\"'\", \"\")), ' ', text) # don't remove ' in words\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
        "    # Single character removal\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "    #text = ' '.join(segment(text))\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_preprocessing_no_lemmatizer(text):\n",
        "\n",
        "    nopunc = clean_text_no_smiley(text)\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "\n",
        "    combined_text = ' '.join(tokenized_text)\n",
        "    return combined_text\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RNknW6QdxmJ",
        "colab_type": "code",
        "outputId": "90aea26b-4056-465c-ec61-3033476359dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "try:\n",
        "    train = pd.read_csv('input/preprocessed_train.csv')\n",
        "    print('Training data shape: ', train.shape)\n",
        "    test = pd.read_csv('input/preprocessed_test.csv')\n",
        "    print('Testing data shape: ', test.shape)\n",
        "except:\n",
        "    train = pd.read_csv('drive/My Drive/NLP_data/train.csv')\n",
        "    print('Training data shape: ', train.shape)\n",
        "    test = pd.read_csv('drive/My Drive/NLP_data/test.csv')\n",
        "    print('Testing data shape: ', test.shape)\n",
        "\n",
        "    train['text'] = train['text'].apply(\n",
        "        lambda x: text_preprocessing_no_lemmatizer(x))\n",
        "    test['text'] = test['text'].apply(\n",
        "        lambda x: text_preprocessing_no_lemmatizer(x))\n",
        "\n",
        "    train.drop([\"keyword\", \"location\"], axis=1, inplace=True)\n",
        "    test.drop([\"keyword\", \"location\"], axis=1, inplace=True)\n",
        "\n",
        "    train.to_csv('input/preprocessed_train.csv')\n",
        "    test.to_csv('input/preprocessed_test.csv')\n",
        "\n",
        "\n",
        "train[['text']].head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (7613, 4)\n",
            "Testing data shape:  (3263, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>our deeds are the reason of this earthquake may allah forgive us all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>people receive wildfires evacuation orders in california</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>just got sent this photo from ruby alaska as smoke from wildfires pours into school</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                 text\n",
              "0                                                                our deeds are the reason of this earthquake may allah forgive us all\n",
              "1                                                                                               forest fire near la ronge sask canada\n",
              "2  all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\n",
              "3                                                                            people receive wildfires evacuation orders in california\n",
              "4                                                 just got sent this photo from ruby alaska as smoke from wildfires pours into school"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjaNY9s1dxmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom dataset\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, input_ids, segment_ids, labels=None):\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.input_ids))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return (self.input_ids[i], self.segment_ids[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZLJfHXq-6Ds",
        "colab_type": "code",
        "outputId": "67d003f3-89eb-429f-ab8c-3be8e7ce7ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_type = 'bert-large-uncased'\n",
        "bertTokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "max_len_train = 0\n",
        "max_len_test = 0\n",
        "\n",
        "# Find the longest sentence\n",
        "for sentence in train[\"text\"]:\n",
        "    input_ids = bertTokenizer.encode(sentence, add_special_tokens=True)\n",
        "    max_len_train = max(max_len_train, len(input_ids))\n",
        "\n",
        "for sentence in test[\"text\"]:\n",
        "    input_ids = bertTokenizer.encode(sentence, add_special_tokens=True)\n",
        "    max_len_test = max(max_len_test, len(input_ids))\n",
        "\n",
        "print('Max sentence length in training data: ', max_len_train)\n",
        "print('Max sentence length in testing data: ', max_len_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length in training data:  41\n",
            "Max sentence length in testing data:  39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6_uanY5QzNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrmFNv5XdxmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_df_to_BERT_input(sequences, bertTokenizer):\n",
        "    input_ids = []\n",
        "    segment_ids = []\n",
        "    attention_masks = []\n",
        "  \n",
        "    for index, sequence in enumerate(sequences):\n",
        "        # Split the sentence into tokens.\n",
        "        encoded_dict = bertTokenizer.encode_plus(sequence, add_special_tokens=True, max_length=max_len, \n",
        "                                              pad_to_max_length=True,\n",
        "                                              return_attention_mask = True,\n",
        "                                              return_token_type_ids = True,\n",
        "                                              return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                              )   \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "        segment_ids.append(encoded_dict['token_type_ids'])\n",
        "        \n",
        "    return (input_ids, attention_masks, segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm6hVWCqdxmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bert_input = convert_df_to_BERT_input(train[\"text\"])\n",
        "input_ids, attention_masks, segment_ids = convert_df_to_BERT_input(train['text'], bertTokenizer)\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "segment_ids = torch.cat(segment_ids, dim=0)\n",
        "labels = torch.tensor(train['target'], dtype=torch.long) \n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7qFR55K_xG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "embedding_dataloader = DataLoader(\n",
        "            dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27eB98CXdxmg",
        "colab_type": "code",
        "outputId": "69e3f723-266e-4c19-98e8-448c04f36283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bert_model = BertModel.from_pretrained(model_type, output_hidden_states=True)\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "bert_model.to(device)\n",
        "bert_model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 1024)\n",
              "    (token_type_embeddings): Embedding(2, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMQS8mrydxmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embeddings(embedding_dataloader):\n",
        "  bert_embeddings = None\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in embedding_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "          inputs = {'input_ids': batch[0],\n",
        "                    'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2]\n",
        "                    }\n",
        "          _, _, hidden_states = bert_model(**inputs)\n",
        "          # hidden states length is 13 (1 embedding + 12 hidden layers)\n",
        "\n",
        "          # second to last \n",
        "          hidden_layer = hidden_states[-2]\n",
        "\n",
        "          if bert_embeddings is None:\n",
        "            bert_embeddings = torch.mean(hidden_layer, dim=1)\n",
        "          else:\n",
        "            bert_embeddings = torch.cat([bert_embeddings, torch.mean(hidden_layer, dim=1)], dim=0)\n",
        "\n",
        "  return bert_embeddings                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSDxzwJ7dxmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_embeddings = create_embeddings(embedding_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8UPwjJmkY2L",
        "colab_type": "code",
        "outputId": "b7a21735-c364-4993-dc6c-77df09a80224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(bert_embeddings))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMgdUr6Edxmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = train[\"target\"]\n",
        "train_data, validation_data, train_target, validation_target = train_test_split(\n",
        "   bert_embeddings, labels, test_size=0.2, random_state=1000)\n",
        "test_data = test[\"text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci9M5Z2zdxmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(train_data, train_target)\n",
        "validation_dataset = TensorDataset(validation_data, validation_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH5AuaZLYQFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            validation_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orfvGyPodxmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "seq_length = 768 if model_type == \"bert-base-uncased\" else 1024\n",
        "hidden_dim = seq_length\n",
        "learning_rate = 1e-5 #1e-5\n",
        "num_epochs = 10\n",
        "patience = 2\n",
        "num_layers = 2\n",
        "is_bidirectional = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNJoOdXKk8hr",
        "colab_type": "code",
        "outputId": "b4f66300-baaa-4283-cbd2-acd1fa62af58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wim9iMXHdxm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwitterClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, bidirectional):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, \n",
        "                            num_layers=num_layers, bidirectional=bidirectional)\n",
        "        \n",
        "        \n",
        "        #hidden_dim * num_layers * (1+self.config.bidirectional), 2\n",
        "        linear_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.fc = nn.Linear(linear_dim, 2)\n",
        "          \n",
        "    def forward(self, sentence):\n",
        "        # input shape: seq_len, batch, input_size\n",
        "        packed_output, (hidden, cell) = self.lstm(sentence.view(1, sentence.shape[0], -1))\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "        #output, hidden = self.dropout(lstm_out.view(1, -1))\n",
        "        y = self.fc(hidden)\n",
        "        log_probs = F.log_softmax(y.squeeze(0), dim=0)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EVs6FsLdxm8",
        "colab_type": "code",
        "outputId": "1ea831ad-581e-4a51-e148-a7d3b8eafe94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = TwitterClassifier(seq_length, hidden_dim, num_layers, is_bidirectional).to(device)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TwitterClassifier(\n",
              "  (lstm): LSTM(1024, 1024, num_layers=2, bidirectional=True)\n",
              "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27i07KCcoN6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def class_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    \"\"\"\n",
        "    rounded_preds = preds.argmax(1)\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    #target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
        "    #print(classification_report(rounded_preds.cpu().numpy(), y.cpu().numpy(), target_names=target_names))\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u-3tnF0dxnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    all_predictions = torch.tensor([], dtype=torch.long);\n",
        "    all_labels = torch.tensor([], dtype=torch.long);\n",
        "    for batch in iterator:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        predictions = model(batch[0])\n",
        "\n",
        "        loss = criterion(predictions, batch[1])\n",
        "        \n",
        "        acc = class_accuracy(predictions, batch[1])\n",
        "        \n",
        "        all_predictions = torch.cat((all_predictions, predictions.detach().cpu().argmax(1)), dim=0)\n",
        "        all_labels = torch.cat((all_labels, batch[1].detach().cpu()), dim=0)\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    f1 = f1_score(all_predictions, all_labels)\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn4XYvVQh-Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    all_predictions = torch.tensor([], dtype=torch.long);\n",
        "    all_labels = torch.tensor([], dtype=torch.long);\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            predictions = model(batch[0])\n",
        "            loss = criterion(predictions, batch[1])\n",
        "            \n",
        "            acc = class_accuracy(predictions, batch[1])\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predictions.detach().cpu().argmax(1)), dim=0)\n",
        "            all_labels = torch.cat((all_labels, batch[1].detach().cpu()), dim=0)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_score(all_predictions, all_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWfaidu4dxnH",
        "colab_type": "code",
        "outputId": "7944ed89-7a26-4233-aca7-6a173086378c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "import time\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "criterion = nn.NLLLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "all_train_loss = []\n",
        "all_valid_loss = []\n",
        "all_train_acc = []\n",
        "all_valid_acc = []\n",
        "\n",
        "early_stop = 0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc, val_f1 = evaluate(model, validation_dataloader, criterion)\n",
        "\n",
        "    all_train_loss.append(train_loss)\n",
        "    all_train_acc.append(train_acc)\n",
        "    all_valid_loss.append(valid_loss)\n",
        "    all_valid_acc.append(valid_acc)\n",
        "    '''\n",
        "    if best_F1 < valid_F1:\n",
        "      early_stop = 0\n",
        "      best_f1 = valid_F1\n",
        "    else:\n",
        "      early_stop += 1\n",
        "\n",
        "    if early_stop >= patience:\n",
        "      break\n",
        "'''\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    print(f'\\t Train F1: {train_f1:.3f} |')\n",
        "    print(f'\\t Val. F1: {val_f1:.3f} |')\n",
        "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.389 | Train Acc: 74.29%\n",
            "\t Val. Loss: 3.269 |  Val. Acc: 78.32%\n",
            "\t Train F1: 0.700 |\n",
            "\t Val. F1: 0.738 |\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.271 | Train Acc: 78.08%\n",
            "\t Val. Loss: 3.225 |  Val. Acc: 81.31%\n",
            "\t Train F1: 0.741 |\n",
            "\t Val. F1: 0.775 |\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.242 | Train Acc: 79.84%\n",
            "\t Val. Loss: 3.207 |  Val. Acc: 82.81%\n",
            "\t Train F1: 0.763 |\n",
            "\t Val. F1: 0.793 |\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.227 | Train Acc: 80.48%\n",
            "\t Val. Loss: 3.196 |  Val. Acc: 82.68%\n",
            "\t Train F1: 0.769 |\n",
            "\t Val. F1: 0.793 |\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.216 | Train Acc: 80.66%\n",
            "\t Val. Loss: 3.187 |  Val. Acc: 83.00%\n",
            "\t Train F1: 0.774 |\n",
            "\t Val. F1: 0.795 |\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.207 | Train Acc: 81.22%\n",
            "\t Val. Loss: 3.181 |  Val. Acc: 82.48%\n",
            "\t Train F1: 0.780 |\n",
            "\t Val. F1: 0.788 |\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.203 | Train Acc: 81.52%\n",
            "\t Val. Loss: 3.178 |  Val. Acc: 82.74%\n",
            "\t Train F1: 0.783 |\n",
            "\t Val. F1: 0.792 |\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.194 | Train Acc: 81.95%\n",
            "\t Val. Loss: 3.176 |  Val. Acc: 82.74%\n",
            "\t Train F1: 0.787 |\n",
            "\t Val. F1: 0.793 |\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.188 | Train Acc: 82.18%\n",
            "\t Val. Loss: 3.173 |  Val. Acc: 83.13%\n",
            "\t Train F1: 0.790 |\n",
            "\t Val. F1: 0.796 |\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 3.184 | Train Acc: 82.29%\n",
            "\t Val. Loss: 3.172 |  Val. Acc: 82.81%\n",
            "\t Train F1: 0.792 |\n",
            "\t Val. F1: 0.792 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OjBc1tV1QoS",
        "colab_type": "code",
        "outputId": "600172b1-4a11-4516-f064-557eab2234ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(all_train_loss, label='train')\n",
        "plt.plot(all_valid_loss, label='validation')\n",
        "plt.legend()\n",
        "plt.plot(all_train_acc, label='train')\n",
        "plt.plot(all_valid_acc, label='validation')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb5b0806320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH3H45onRxZE8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaUvKinsi6St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bert_input = convert_df_to_BERT_input(train[\"text\"])\n",
        "input_ids, attention_masks, segment_ids = convert_df_to_BERT_input(test['text'], bertTokenizer)\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "segment_ids = torch.cat(segment_ids, dim=0)\n",
        "\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkpgJMjCvFtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dataloader = DataLoader(\n",
        "            dataset,\n",
        "            sampler = SequentialSampler(dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqnJ5SGopXwk",
        "colab_type": "code",
        "outputId": "90756c88-8a57-400b-fbaf-827b4266785a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test_bert_embeddings = create_embeddings(embedding_dataloader)\n",
        "test_dataset = TensorDataset(test_bert_embeddings)\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, \n",
        "            sampler = SequentialSampler(test_dataset), \n",
        "            batch_size = batch_size\n",
        "        )\n",
        "'''\n",
        "data = DataLoader(bert_input)\n",
        "bert_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for index, (input_ids, segment_ids) in enumerate(data):\n",
        "        tokens_tensor = torch.tensor([input_ids])\n",
        "        segments_tensors = torch.tensor([segment_ids])\n",
        "        _, _, hidden_states = bert_model(tokens_tensor, segments_tensors)\n",
        "        # hidden states length is 13 (1 embedding + 12 hidden layers)\n",
        "        hidden_layers = hidden_states[1:]\n",
        "        \n",
        "        token_vecs = hidden_layers[-1][0]\n",
        "\n",
        "        # Calculate the average of all token vectors.\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        bert_embeddings.append(sentence_embedding)            \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndata = DataLoader(bert_input)\\nbert_embeddings = []\\nwith torch.no_grad():\\n    for index, (input_ids, segment_ids) in enumerate(data):\\n        tokens_tensor = torch.tensor([input_ids])\\n        segments_tensors = torch.tensor([segment_ids])\\n        _, _, hidden_states = bert_model(tokens_tensor, segments_tensors)\\n        # hidden states length is 13 (1 embedding + 12 hidden layers)\\n        hidden_layers = hidden_states[1:]\\n        \\n        token_vecs = hidden_layers[-1][0]\\n\\n        # Calculate the average of all token vectors.\\n        sentence_embedding = torch.mean(token_vecs, dim=0)\\n        bert_embeddings.append(sentence_embedding)            \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YZwE_iZo8pV",
        "colab_type": "code",
        "outputId": "21b9121f-4e12-443d-f3d7-dbc687880ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def predict_func(iterator):\n",
        "    predictions = torch.tensor([], dtype=torch.long)\n",
        "    for batch in iterator:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input = batch[0]\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            predictions = torch.cat((predictions, output.detach().cpu().argmax(1)))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "predictions = predict_func(test_dataloader)\n",
        "print(len(predictions))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsfJ39EVygcI",
        "colab_type": "code",
        "outputId": "5cc84c44-8b03-40ee-a5da-c44075b68a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(predictions[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2yhlG8epa2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def submission(submission_file_path,submission_data):\n",
        "    sample_submission = pd.read_csv(submission_file_path)\n",
        "    sample_submission[\"target\"] = [data.item() for data in submission_data]\n",
        "    print(sample_submission[\"target\"])\n",
        "    sample_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDAayQKCp0CV",
        "colab_type": "code",
        "outputId": "647dad3d-e285-43f4-d175-1036661777f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "submission_file_path = \"drive/My Drive/NLP_data/sample_submission.csv\"\n",
        "submission(submission_file_path,predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "3258    0\n",
            "3259    1\n",
            "3260    1\n",
            "3261    1\n",
            "3262    1\n",
            "Name: target, Length: 3263, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycO0ptsErEka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}