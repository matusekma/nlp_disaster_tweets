{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (7613, 5)\n",
      "Testing data shape:  (3263, 4)\n",
      "         id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
      "1                Forest fire near La Ronge Sask. Canada       1  \n",
      "2     All residents asked to 'shelter in place' are ...       1  \n",
      "3     13,000 people receive #wildfires evacuation or...       1  \n",
      "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  Two giant cranes holding a bridge collapse int...       1  \n",
      "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
      "7611  Police investigating after an e-bike collided ...       1  \n",
      "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
      "\n",
      "[61 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "print('Testing data shape: ', test.shape)\n",
    "print(train[train.keyword.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                   8105\n",
      "keyword                                           rescued\n",
      "location                                   The Multiverse\n",
      "text        But now #Skyrim awaits to be rescued...again.\n",
      "target                                                  0\n",
      "Name: 5680, dtype: object\n",
      "         id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
      "1                Forest fire near La Ronge Sask. Canada       1  \n",
      "2     All residents asked to 'shelter in place' are ...       1  \n",
      "3     13,000 people receive #wildfires evacuation or...       1  \n",
      "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  Two giant cranes holding a bridge collapse int...       1  \n",
      "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
      "7611  Police investigating after an e-bike collided ...       1  \n",
      "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
      "\n",
      "[7613 rows x 5 columns]\n",
      "         id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0            deed reason earthquake may allah forgive u       1  \n",
      "1                 forest fire near la ronge sask canada       1  \n",
      "2     resident ask shelter place notify officer evac...       1  \n",
      "3     <number> people receive wildfire evacuation or...       1  \n",
      "4     get sent photo ruby alaska smoke wildfires pou...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  two giant crane hold bridge collapse nearby ho...       1  \n",
      "7609  <user> <user> control wild fire california eve...       1  \n",
      "7610  <number> <number> utc <number> km volcano hawa...       1  \n",
      "7611  police investigate e bike collide car little p...       1  \n",
      "7612  latest home raze northern california wildfire ...       1  \n",
      "\n",
      "[7613 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO - clean text\n",
    "# Applying a first round of text cleaning techniques\n",
    "import re, string\n",
    "def clean_text(text):\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"<URL>\", text)    \n",
    "    text = re.sub(\"/\",\" / \", text)\n",
    "    text = re.sub('@(\\w+)', '<USER>', text)\n",
    "    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', \"<SMILE>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}p+', \"<LOLFACE>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}\\(+|\\)+#{nose}#{eyes}', \"<SADFACE>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}[\\/|l*]', \"<NEUTRALFACE>\", text)\n",
    "    text = re.sub('<3',\"<HEART>\", text)\n",
    "    text = re.sub('[-+]?[.\\d]*[\\d]+[:,.\\d]*', \"<NUMBER>\", text)\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    #text = re.sub('\\[.*?\\]', '', text)\n",
    "    #text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation.replace('<', '').replace('>', '')), ' ', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    #text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    \n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def text_preprocessing(text):\n",
    "   \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer() \n",
    "  \n",
    "    nopunc = clean_text(text)\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    \n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(remove_stopwords)]\n",
    "    \n",
    "    combined_text = ' '.join(lemmatized)\n",
    "    return combined_text\n",
    "\n",
    "print(train.iloc[5680])\n",
    "print(train)\n",
    "train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          8105\n",
      "keyword                  rescued\n",
      "location          The Multiverse\n",
      "text        skyrim awaits rescue\n",
      "target                         0\n",
      "Name: 5680, dtype: object\n",
      "id           43\n",
      "keyword     NaN\n",
      "location    NaN\n",
      "text           \n",
      "Name: 13, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(train.iloc[5680])\n",
    "print(test.iloc[13])\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.92158002 -0.054151   -1.00610006 ... -0.6692     -0.49597999\n",
      "   0.18621001]\n",
      " [ 0.48737001  0.16796    -0.41657999 ... -0.65139002 -0.064736\n",
      "   0.75953001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.41821    -0.76490003  0.49147999 ... -0.48903999  0.33109999\n",
      "   0.74254   ]\n",
      " [-0.48291001 -0.0029234  -1.59609997 ...  0.44295999 -0.29177001\n",
      "   0.47444001]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train[\"text\"])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#print(tokenizer.word_index)\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "        '../input/glove.twitter.27B.50d.txt',\n",
    "    #'../input/glove.6B.50d.txt',\n",
    "    tokenizer.word_index, embedding_dim)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8542814221331998"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11982, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          8105\n",
      "keyword                  rescued\n",
      "location          The Multiverse\n",
      "text        skyrim awaits rescue\n",
      "target                         0\n",
      "Name: 5680, dtype: object\n",
      "id           43\n",
      "keyword     NaN\n",
      "location    NaN\n",
      "text           \n",
      "Name: 13, dtype: object\n",
      "11982\n"
     ]
    }
   ],
   "source": [
    "print(train.iloc[5680])\n",
    "print(test.iloc[13])\n",
    "\n",
    "train[\"text\"] = tokenizer.texts_to_sequences(train[\"text\"].values)\n",
    "test[\"text\"] = tokenizer.texts_to_sequences(test[\"text\"].values)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train.iloc[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: [0] if x == [] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id keyword location text\n",
      "0    NaN     NaN      NaN  NaN\n",
      "1    NaN     NaN      NaN  NaN\n",
      "2    NaN     NaN      NaN  NaN\n",
      "3    NaN     NaN      NaN  NaN\n",
      "4    NaN     NaN      NaN  NaN\n",
      "...   ..     ...      ...  ...\n",
      "3258 NaN     NaN      NaN  NaN\n",
      "3259 NaN     NaN      NaN  NaN\n",
      "3260 NaN     NaN      NaN  NaN\n",
      "3261 NaN     NaN      NaN  NaN\n",
      "3262 NaN     NaN      NaN  NaN\n",
      "\n",
      "[3263 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test[train[train.astype(str)['text'] == '[]']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert word ids to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokens_to_averaged_vectors(tokens, embedding_matrix):\\n    vectors = np.asarray([np.asarray(embedding_matrix[token]) for token in tokens])\\n    return vectors.mean(axis=0)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tokens_to_averaged_vectors(tokens, embedding_matrix):\n",
    "    vectors = np.asarray([np.asarray(embedding_matrix[token]) for token in tokens])\n",
    "    return vectors.mean(axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_text = train[\"text\"].apply(lambda x: tokens_to_averaged_vectors(x, embedding_matrix))\n",
    "test_text = test[\"text\"].apply(lambda x: tokens_to_averaged_vectors(x, embedding_matrix))\n",
    "'''\n",
    "\n",
    "train_text = train[\"text\"]\n",
    "test_text = test[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = train[\"target\"]\n",
    "train_data, validation_data, train_target, validation_target = train_test_split(\n",
    "   train_text, target, test_size=0.2, random_state=1000)\n",
    "test_data = test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1523,), (6090,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_target.shape, train_target.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def create_emb_layer(weights_matrix):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.EmbeddingBag.from_pretrained(weights_matrix, freeze=True) \n",
    "    \n",
    "    #if non_trainable:\n",
    "        #emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class TwitterClassifier(nn.Module):\n",
    "    def __init__(self, weights_matrix):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix)\n",
    "        self.fc = nn.Linear(embedding_dim, 2)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6101                              [479, 10681, 1765, 332]\n",
      "3298    [8321, 617, 2618, 835, 289, 796, 2188, 94, 103...\n",
      "6817    [3, 306, 134, 257, 110, 134, 1120, 2309, 934, ...\n",
      "3801                   [8782, 205, 1847, 574, 8, 2327, 1]\n",
      "7088    [5194, 1415, 1306, 1062, 5195, 1633, 1476, 361...\n",
      "                              ...                        \n",
      "7419                  [35, 328, 99, 268, 70, 770, 319, 1]\n",
      "3776    [8, 108, 8759, 2040, 8760, 407, 8, 1149, 924, ...\n",
      "6215    [204, 10816, 144, 891, 794, 1052, 2, 10817, 20...\n",
      "4695    [3548, 14, 196, 1325, 97, 1147, 644, 3548, 326...\n",
      "1459                              [1061, 141, 84, 872, 1]\n",
      "Name: text, Length: 6090, dtype: object\n",
      "(tensor([  479, 10681,  1765,   332]), 0)\n",
      "tensor([ 222, 1614,   54,   29])\n"
     ]
    }
   ],
   "source": [
    "# custom dataset\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, transforms=None):\n",
    "        self.X = texts\n",
    "        if labels is not None:\n",
    "            self.y = np.asarray(labels)\n",
    "        else:\n",
    "            self.y = None\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X.iloc[i]\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            return (data, self.y[i])\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "print(train_data)\n",
    "train_data = TwitterDataset(train_data, train_target)\n",
    "validation_data = TwitterDataset(validation_data, validation_target)\n",
    "test_data = TwitterDataset(test_data)\n",
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11982, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwitterClassifier(\n",
       "  (embedding): EmbeddingBag(11982, 50, mode=mean)\n",
       "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TwitterClassifier(torch.tensor(embedding_matrix, dtype=torch.float)).to(device)\n",
    "#model = TwitterClassifier(torch.tensor(embedding_matrix, dtype=torch.float)).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[1] for entry in batch])\n",
    "    text = [entry[0] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_rec_F1(labels, preds):\n",
    "    # true positives\n",
    "    tp = 0\n",
    "    # false negatives\n",
    "    fn = 0\n",
    "    for label, pred in zip(labels, preds):\n",
    "        if label == 1:\n",
    "            if pred == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                \n",
    "    pospreds = sum(preds)\n",
    "    precision = tp / pospreds\n",
    "    recall = tp / (fn + tp)\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return (precision, recall, 0.0)\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    # dataloaders\n",
    "    data = DataLoader(sub_train_, shuffle=True)\n",
    "    for i, (text, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.argmax(1)\n",
    "        train_acc += (pred == cls).sum().item()\n",
    "        labels.append(cls.item())\n",
    "        preds.append(pred.item())\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return prec_rec_F1(labels, preds)\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def validate_func(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    data = DataLoader(data_)\n",
    "    for text, cls in data:\n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            pred = output.argmax(1)\n",
    "            acc += (pred == cls).sum().item()\n",
    "            labels.append(cls.item())\n",
    "            preds.append(pred.item())\n",
    "            \n",
    "    return prec_rec_F1(labels, preds)\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.68 (train)\n",
      "\t\tF1 score: 0.72 (valid)\n",
      "Epoch: 2  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.70 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 3  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.70 (train)\n",
      "\t\tF1 score: 0.72 (valid)\n",
      "Epoch: 4  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.70 (train)\n",
      "\t\tF1 score: 0.74 (valid)\n",
      "Epoch: 5  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.70 (train)\n",
      "\t\tF1 score: 0.71 (valid)\n",
      "Epoch: 6  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 7  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 8  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.70 (valid)\n",
      "Epoch: 9  | time in 0 minutes, 20 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.72 (valid)\n",
      "Epoch: 10  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.74 (valid)\n",
      "Epoch: 11  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 12  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.72 (train)\n",
      "\t\tF1 score: 0.72 (valid)\n",
      "Epoch: 13  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.72 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 14  | time in 0 minutes, 19 seconds\n",
      "\t\tF1 score: 0.71 (train)\n",
      "\t\tF1 score: 0.74 (valid)\n",
      "Epoch: 15  | time in 0 minutes, 18 seconds\n",
      "\t\tF1 score: 0.72 (train)\n",
      "\t\tF1 score: 0.70 (valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 15\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "sub_train_, sub_valid_ = train_data, validation_data\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_precision, train_recall, train_F1 = train_func(sub_train_)\n",
    "    valid_precision, valid_recall, valid_F1 = validate_func(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\t\\tF1 score: {train_F1:.2f} (train)\\n\\t\\tF1 score: {valid_F1:.2f} (valid)')\n",
    "    #print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    #print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    }
   ],
   "source": [
    "def predict_func(test_data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    predictions = []\n",
    "    data = DataLoader(test_data_)\n",
    "    for text in data:\n",
    "        text = text.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            predictions.append(output.argmax(1))\n",
    "            #acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = predict_func(test_data)\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,submission_data):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = [tensor.numpy()[0] for tensor in submission_data]\n",
    "    print(sample_submission[\"target\"])\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-b77793aa0145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msubmission_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../input/sample_submission.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-77c4eafaf82a>\u001b[0m in \u001b[0;36msubmission\u001b[1;34m(submission_file_path, submission_data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubmission_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubmission_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-77c4eafaf82a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubmission_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubmission_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "submission_file_path = \"../input/sample_submission.csv\"\n",
    "submission(submission_file_path,predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
