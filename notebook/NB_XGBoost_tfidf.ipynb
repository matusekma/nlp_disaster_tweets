{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import itertools\n",
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# sklearn \n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "\n",
    "# from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Applying a first round of text cleaning techniques\n",
    "import re, string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\" \", text)    \n",
    "    \n",
    "    text = re.sub(\"/\",\" / \", text)\n",
    "    text = re.sub('@(\\w+)', '', text)\n",
    "    \n",
    "    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', \"<smile>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}p+', \"<lolface>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}\\(+|\\)+#{nose}#{eyes}', \"<sadface>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}[\\/|l*]', \"<neutralface>\", text)\n",
    "    text = re.sub('<3',\"<heart>\", text)\n",
    "    # numbers\n",
    "    text = re.sub('[-+]?[.\\d]*[\\d]+[:,.\\d]*', \" \", text)\n",
    "    \n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    #text = re.sub('\\[.*?\\]', '', text)\n",
    "    #text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation.replace('<', '').replace('>', '')), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    \n",
    "    #text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n",
    "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    \n",
    "    #text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    \n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def text_preprocessing(text):\n",
    "   \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer() \n",
    "  \n",
    "    nopunc = clean_text(text)\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    \n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(remove_stopwords)]\n",
    "    \n",
    "    combined_text = ' '.join(lemmatized)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data or preprocessed data if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    train = pd.read_csv(\\'../input/preprocessed_train.csv\\')\\n    print(\\'Preprocessed training data shape: \\', train.shape)\\n    test = pd.read_csv(\\'../input/preprocessed_test.csv\\')\\n    print(\\'Preprocessed testing data shape: \\', test.shape)\\n    \\nexcept:\\n    train = pd.read_csv(\\'../input/train.csv\\')\\n    print(\\'Training data shape: \\', train.shape)\\n    test = pd.read_csv(\\'../input/test.csv\\')\\n    print(\\'Testing data shape: \\', test.shape)\\n    \\n    train[\\'text\\'] = train[\\'text\\'].apply(lambda x: text_preprocessing(x))\\n    test[\\'text\\'] = test[\\'text\\'].apply(lambda x: text_preprocessing(x))\\n    train.to_csv(\\'../input/preprocessed_train.csv\\')\\n    test.to_csv(\\'../input/preprocessed_test.csv\\')\\n\\ntrain.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\\ntest.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "try:\n",
    "    train = pd.read_csv('../input/preprocessed_train.csv')\n",
    "    print('Preprocessed training data shape: ', train.shape)\n",
    "    test = pd.read_csv('../input/preprocessed_test.csv')\n",
    "    print('Preprocessed testing data shape: ', test.shape)\n",
    "    \n",
    "except:\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "    print('Training data shape: ', train.shape)\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "    print('Testing data shape: ', test.shape)\n",
    "    \n",
    "    train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "    test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "    train.to_csv('../input/preprocessed_train.csv')\n",
    "    test.to_csv('../input/preprocessed_test.csv')\n",
    "\n",
    "train.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "test.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (7613, 5)\n",
      "Testing data shape:  (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "print('Testing data shape: ', test.shape)\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "\n",
    "train.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "test.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "\n",
    "train.to_csv('../input/preprocessed_train.csv')\n",
    "test.to_csv('../input/preprocessed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tfidf = tfidf.fit_transform(train['text'])\n",
    "test_tfidf = tfidf.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57666345, 0.59195894, 0.62900506, 0.61439114, 0.74363057])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf_NB_TFIDF = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49414271, 0.42026266, 0.45822994, 0.38115632, 0.59536542])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "              nthread=10, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb_TFIDF.fit(train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44540541, 0.34349593, 0.43854996, 0.32488479, 0.51889683])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_gb_TFIDF = GradientBoostingClassifier(n_estimators=100)\n",
    "scores = model_selection.cross_val_score(clf_gb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gb_TFIDF.fit(train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,model,test_vectors):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = \"../input/sample_submission.csv\"\n",
    "#submission(submission_file_path,clf_NB_TFIDF,test_tfidf)\n",
    "submission(submission_file_path,clf_gb_TFIDF,test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_rec_F1(labels, preds):\n",
    "    # true positives\n",
    "    tp = 0\n",
    "    # false negatives\n",
    "    fn = 0\n",
    "    for label, pred in zip(labels, preds):\n",
    "        if label == 1:\n",
    "            if pred == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                \n",
    "    pospreds = sum(preds)\n",
    "    precision = tp / pospreds\n",
    "    recall = tp / (fn + tp)\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return (precision, recall, 0.0)\n",
    "    return (precision, recall, f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
