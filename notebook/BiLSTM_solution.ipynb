{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import itertools\n",
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Applying a first round of text cleaning techniques\n",
    "import re, string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\" \", text)    \n",
    "    \n",
    "    text = re.sub(\"/\",\" / \", text)\n",
    "    text = re.sub('@(\\w+)', '', text)\n",
    "    \n",
    "    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', \"<smile>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}p+', \"<lolface>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}\\(+|\\)+#{nose}#{eyes}', \"<sadface>\", text)\n",
    "    text = re.sub('#{eyes}#{nose}[\\/|l*]', \"<neutralface>\", text)\n",
    "    text = re.sub('<3',\"<heart>\", text)\n",
    "    # numbers\n",
    "    text = re.sub('[-+]?[.\\d]*[\\d]+[:,.\\d]*', \" \", text)\n",
    "    \n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    #text = re.sub('\\[.*?\\]', '', text)\n",
    "    #text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation.replace('<', '').replace('>', '')), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    \n",
    "    #text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n",
    "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    \n",
    "    #text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    \n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def text_preprocessing(text):\n",
    "   \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer() \n",
    "  \n",
    "    nopunc = clean_text(text)\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    \n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(remove_stopwords)]\n",
    "    \n",
    "    combined_text = ' '.join(lemmatized)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data or preprocessed data if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    train = pd.read_csv(\\'../input/preprocessed_train.csv\\')\\n    print(\\'Preprocessed training data shape: \\', train.shape)\\n    test = pd.read_csv(\\'../input/preprocessed_test.csv\\')\\n    print(\\'Preprocessed testing data shape: \\', test.shape)\\n    \\nexcept:\\n    train = pd.read_csv(\\'../input/train.csv\\')\\n    print(\\'Training data shape: \\', train.shape)\\n    test = pd.read_csv(\\'../input/test.csv\\')\\n    print(\\'Testing data shape: \\', test.shape)\\n    \\n    train[\\'text\\'] = train[\\'text\\'].apply(lambda x: text_preprocessing(x))\\n    test[\\'text\\'] = test[\\'text\\'].apply(lambda x: text_preprocessing(x))\\n    train.to_csv(\\'../input/preprocessed_train.csv\\')\\n    test.to_csv(\\'../input/preprocessed_test.csv\\')\\n\\ntrain.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\\ntest.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "try:\n",
    "    train = pd.read_csv('../input/preprocessed_train.csv')\n",
    "    print('Preprocessed training data shape: ', train.shape)\n",
    "    test = pd.read_csv('../input/preprocessed_test.csv')\n",
    "    print('Preprocessed testing data shape: ', test.shape)\n",
    "    \n",
    "except:\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "    print('Training data shape: ', train.shape)\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "    print('Testing data shape: ', test.shape)\n",
    "    \n",
    "    train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "    test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "    train.to_csv('../input/preprocessed_train.csv')\n",
    "    test.to_csv('../input/preprocessed_test.csv')\n",
    "\n",
    "train.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "test.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (7613, 5)\n",
      "Testing data shape:  (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "print('Testing data shape: ', test.shape)\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "\n",
    "train.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "test.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
    "\n",
    "train.to_csv('../input/preprocessed_train.csv')\n",
    "test.to_csv('../input/preprocessed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.18131     0.52524    -0.28472    ... -0.30759999 -0.42807001\n",
      "   0.68215001]\n",
      " [-0.094661    0.13413     0.21436    ...  0.47062001 -0.44501001\n",
      "   0.13282999]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.41821    -0.76490003  0.49147999 ... -0.48903999  0.33109999\n",
      "   0.74254   ]\n",
      " [-0.48291001 -0.0029234  -1.59609997 ...  0.44295999 -0.29177001\n",
      "   0.47444001]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train[\"text\"])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#print(tokenizer.word_index)\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "        '../input/glove.twitter.27B.50d.txt',\n",
    "    #'../input/glove.6B.50d.txt',\n",
    "    tokenizer.word_index, embedding_dim)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8697109342270632"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11935, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        8105\n",
      "text      skyrim awaits rescue\n",
      "target                       0\n",
      "Name: 5680, dtype: object\n",
      "id      43\n",
      "text      \n",
      "Name: 13, dtype: object\n",
      "11935\n"
     ]
    }
   ],
   "source": [
    "print(train.iloc[5680])\n",
    "print(test.iloc[13])\n",
    "\n",
    "train[\"text\"] = tokenizer.texts_to_sequences(train[\"text\"].values)\n",
    "test[\"text\"] = tokenizer.texts_to_sequences(test[\"text\"].values)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train.iloc[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: [0] if x == [] else x)\n",
    "train['text'] = train['text'].apply(lambda x: [0] if x == [] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, text, target]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(train[train.astype(str)['text'] == '[]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert word ids to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokens_to_averaged_vectors(tokens, embedding_matrix):\\n    vectors = np.asarray([np.asarray(embedding_matrix[token]) for token in tokens])\\n    return vectors.mean(axis=0)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tokens_to_averaged_vectors(tokens, embedding_matrix):\n",
    "    vectors = np.asarray([np.asarray(embedding_matrix[token]) for token in tokens])\n",
    "    return vectors.mean(axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_text = train[\"text\"].apply(lambda x: tokens_to_averaged_vectors(x, embedding_matrix))\n",
    "test_text = test[\"text\"].apply(lambda x: tokens_to_averaged_vectors(x, embedding_matrix))\n",
    "'''\n",
    "\n",
    "train_text = train[\"text\"]\n",
    "test_text = test[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = train[\"target\"]\n",
    "train_data, validation_data, train_target, validation_target = train_test_split(\n",
    "   train_text, target, test_size=0.2, random_state=1000)\n",
    "test_data = test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1523,), (6090,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_target.shape, train_target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embed_size = 50\n",
    "    hidden_layers = 2\n",
    "    hidden_size = 32\n",
    "    bidirectional = True\n",
    "    output_size = 2\n",
    "    max_epochs = 10\n",
    "    lr = 0.25\n",
    "    batch_size = 1\n",
    "    max_sen_len = 20 # Sequence length for RNN\n",
    "    dropout_keep = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def create_emb_layer(weights_matrix):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    #emb_layer = nn.EmbeddingBag(num_embeddings, embedding_dim, sparse=True)\n",
    "    emb_layer = nn.Embedding.from_pretrained(weights_matrix, freeze=True) \n",
    "    \n",
    "    #if non_trainable:\n",
    "        #emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class TwitterClassifier(nn.Module):\n",
    "    def __init__(self, weights_matrix, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix)\n",
    "       \n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
    "                            hidden_size = self.config.hidden_size,\n",
    "                            num_layers = self.config.hidden_layers,\n",
    "                            dropout = self.config.dropout_keep,\n",
    "                            bidirectional = self.config.bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.linear = nn.Linear(\n",
    "            self.config.hidden_size * self.config.hidden_layers * (1+self.config.bidirectional),\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        #self.linear = nn.Linear(embedding_dim, 2)\n",
    "        #self.init_weights()\n",
    "        \n",
    "    def forward(self, textbatch):\n",
    "        text = textbatch[0]\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, (h_n,c_n) = self.lstm(embedded.view(len(text), 1, -1))\n",
    "        final_feature_map = self.dropout(h_n) # shape=(num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        # Convert input to (batch_size, hidden_size * hidden_layers * num_directions) for linear layer\n",
    "        final_feature_map = torch.cat([final_feature_map[i,:,:] for i in range(final_feature_map.shape[0])], dim=1)\n",
    "        #lstm_out, _ = self.lstm(embedded.view(len(text), 1, -1))\n",
    "        linear = self.linear(final_feature_map)\n",
    "    \n",
    "        return linear\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6101                              [471, 10670, 1770, 326]\n",
      "3298    [8339, 603, 2645, 833, 277, 711, 2207, 88, 927...\n",
      "6817    [297, 127, 252, 103, 127, 1117, 2336, 885, 113...\n",
      "3801                      [8781, 185, 1866, 558, 3, 2354]\n",
      "7088    [5267, 1414, 1302, 993, 5268, 1353, 1471, 3643...\n",
      "Name: text, dtype: object\n",
      "(tensor([  471, 10670,  1770,   326]), 0)\n",
      "tensor([ 220, 1616,   51,   18])\n"
     ]
    }
   ],
   "source": [
    "# custom dataset\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, transforms=None):\n",
    "        self.X = texts\n",
    "        if labels is not None:\n",
    "            self.y = np.asarray(labels)\n",
    "        else:\n",
    "            self.y = None\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X.iloc[i]\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            return (data, self.y[i])\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "print(train_data.head())\n",
    "train_data = TwitterDataset(train_data, train_target)\n",
    "validation_data = TwitterDataset(validation_data, validation_target)\n",
    "test_data = TwitterDataset(test_data)\n",
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11935, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwitterClassifier(\n",
       "  (embedding): Embedding(11935, 50)\n",
       "  (lstm): LSTM(50, 32, num_layers=2, dropout=0.8, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = TwitterClassifier(torch.tensor(embedding_matrix, dtype=torch.float), Config()).to(device)\n",
    "#model = TwitterClassifier(torch.tensor(embedding_matrix, dtype=torch.float)).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[1] for entry in batch])\n",
    "    text = [entry[0] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_rec_F1(labels, preds):\n",
    "    # true positives\n",
    "    tp = 0\n",
    "    # false negatives\n",
    "    fn = 0\n",
    "    for label, pred in zip(labels, preds):\n",
    "        if label == 1:\n",
    "            if pred == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                \n",
    "    pospreds = sum(preds)\n",
    "    precision = tp / pospreds\n",
    "    recall = tp / (fn + tp)\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return (precision, recall, 0.0)\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    # dataloaders\n",
    "    data = DataLoader(sub_train_, shuffle=True)\n",
    "    for i, (text, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.argmax(1)\n",
    "        train_acc += (pred == cls).sum().item()\n",
    "        labels.append(cls.item())\n",
    "        preds.append(pred.item())\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return prec_rec_F1(labels, preds)\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def validate_func(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    data = DataLoader(data_)\n",
    "    for text, cls in data:\n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            l = criterion(output, cls)\n",
    "            loss += l.item()\n",
    "            pred = output.argmax(1)\n",
    "            acc += (pred == cls).sum().item()\n",
    "            labels.append(cls.item())\n",
    "            preds.append(pred.item())\n",
    "            \n",
    "    return prec_rec_F1(labels, preds)\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 1 minutes, 13 seconds\n",
      "\t\tF1 score: 0.63 (train)\n",
      "\t\tF1 score: 0.72 (valid)\n",
      "Epoch: 2  | time in 1 minutes, 10 seconds\n",
      "\t\tF1 score: 0.72 (train)\n",
      "\t\tF1 score: 0.74 (valid)\n",
      "Epoch: 3  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.72 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 4  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.74 (train)\n",
      "\t\tF1 score: 0.73 (valid)\n",
      "Epoch: 5  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.74 (train)\n",
      "\t\tF1 score: 0.75 (valid)\n",
      "Epoch: 6  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.74 (train)\n",
      "\t\tF1 score: 0.76 (valid)\n",
      "Epoch: 7  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.75 (train)\n",
      "\t\tF1 score: 0.75 (valid)\n",
      "Epoch: 8  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.75 (train)\n",
      "\t\tF1 score: 0.74 (valid)\n",
      "Epoch: 9  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.76 (train)\n",
      "\t\tF1 score: 0.72 (valid)\n",
      "Epoch: 10  | time in 1 minutes, 9 seconds\n",
      "\t\tF1 score: 0.76 (train)\n",
      "\t\tF1 score: 0.75 (valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 10\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "sub_train_, sub_valid_ = train_data, validation_data\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_precision, train_recall, train_F1 = train_func(sub_train_)\n",
    "    valid_precision, valid_recall, valid_F1 = validate_func(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\t\\tF1 score: {train_F1:.2f} (train)\\n\\t\\tF1 score: {valid_F1:.2f} (valid)')\n",
    "    #print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    #print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    }
   ],
   "source": [
    "def predict_func(test_data_):\n",
    "    predictions = []\n",
    "    data = DataLoader(test_data_)\n",
    "    for text in data:\n",
    "        text = text.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            predictions.append(output.argmax(1))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = predict_func(test_data)\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,submission_data):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = [tensor.cpu().numpy()[0] for tensor in submission_data]\n",
    "    print(sample_submission[\"target\"])\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "3258    1\n",
      "3259    1\n",
      "3260    1\n",
      "3261    1\n",
      "3262    0\n",
      "Name: target, Length: 3263, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "submission_file_path = \"../input/sample_submission.csv\"\n",
    "submission(submission_file_path,predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
