{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'><font size=\"6\" color=\"#F39C12\">Getting started with Natural Language Processing</font></div>\n",
    "<div align='center'><font size=\"5\" color=\"#F39C12\">A general Introduction</font></div>\n",
    "<hr>\n",
    "\n",
    "\n",
    "<p style='text-align:justify'><b>Key Objectives:</b>This notebook comes as a first part to the **Getting started with NLP Notebooks** that I am writing. This notebook explains the concepts of NLP with respect to this current competition. NLP is the field of study that focuses on the interactions between human language and computers. NLP sits at the intersection of computer science, artificial intelligence, and computational linguistics[[source](https://en.wikipedia.org/wiki/Natural_language_processing)]. NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way.</p>\n",
    "\n",
    "<b>Notebooks in this series</b>\n",
    "\n",
    "  <ul>\n",
    "      <li><a href=\"https://www.kaggle.com/parulpandey/getting-started-with-nlp\" target=\"_blank\">Part 1: Getting started with NLP : A General Introduction </a></li>\n",
    "      <li><a href=\"https://www.kaggle.com/parulpandey/getting-started-with-nlp-2-countvectorizer\" target=\"_blank\">Part 2: Getting started with NLP(2)- CountVectorizer </a></li></ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Dataset\n",
    "The datasets contains a set of tweets which have been divided into a training and a test set. The training set contains a target column identifying whether the tweet pertains to a real disasteror not.\n",
    "\n",
    "Our job is to create a ML model to predict whether the test set tweets belong to a disaster or not, in the form of 1 or 0.This is a classic case of a Binary Classification problem. \n",
    "\n",
    "## Understanding the Evaluation Metric\n",
    "\n",
    "Evaluation metrics are used to measure the quality of the statistical or machine learning model.There are many different types of evaluation metrics available to test a model. These include classification accuracy, logarithmic loss etc. For this particluar problem, our submissions will be evaluated using **F1** between the predicted and expected answers. \n",
    "\n",
    "The **F score**, also called the **F1 score** or **F measure**, is a measure of a test’s accuracy. \n",
    "\n",
    "The F score is defined as the weighted harmonic mean of the test’s precision and recall. \n",
    "\n",
    "![](https://imgur.com/nC4QwrO.png)\n",
    "\n",
    "- Precision, also called the positive predictive value, is the proportion of positive results that truly are positive. \n",
    "\n",
    "- Recall, also called sensitivity, is the ability of a test to correctly identify positive results to get the true positive rate. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)\n",
    "*source:https://en.wikipedia.org/wiki/Precision_and_recall*\n",
    "\n",
    "The  F score reaches the best value, meaning perfect precision and recall, at a value of 1. The worst F score, which means lowest precision and lowest recall, would be a value of 0. \n",
    "\n",
    "\n",
    "\n",
    "### Why is this Useful?\n",
    "The F score is used to measure a test’s accuracy, and it balances the use of precision and recall to do it. The F score can provide a more realistic measure of a test’s performance by using both precision and recall. The F score is often used in information retrieval for measuring search, document classification, and query classification performance. \n",
    "          \n",
    "          \n",
    "source: https://deepai.org/machine-learning-glossary-and-terms/f-score          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Importing the necessary libraries](#imports)\n",
    "- [2. Reading the datasets](#reading)\n",
    "- [3. Basic EDA](#eda)\n",
    "- [4. Text data processing](#processing)\n",
    "- [5. Transforming tokens to vectors](#vectorization)\n",
    "- [6. Buiding a Text Classification model](#model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"imports\"></a>1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a128fcf92197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# text processing libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "# sklearn \n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"reading\"></a> 2. Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'train.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"../input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 7613 observations and 5 features including the TARGET (the label we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape:  (3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data \n",
    "test = pd.read_csv('../input/test.csv')\n",
    "print('Testing data shape: ', test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the test data doesn't have the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"eda\"></a> 3. Basic EDA\n",
    "\n",
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values in training set\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns denote the following: \n",
    "\n",
    "- The `text` of a tweet\n",
    "- A `keyword` from that tweet\n",
    "- The `location` the tweet was sent from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values in test set\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of values are missing in the `location` column in both the training and the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Target Column\n",
    "\n",
    "* ** Distribution of the Target Column**\n",
    "\n",
    "We have to predict whether a given tweet is about a real disaster or not. - If so, predict a 1. If not, predict a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff11c9146d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Exploring the Target Column**\n",
    "Let's look at what the disaster and the non disaster tweets look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A disaster tweet\n",
    "disaster_tweets = train[train['target']==1]['text']\n",
    "disaster_tweets.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love fruits'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not a disaster tweet\n",
    "non_disaster_tweets = train[train['target']==0]['text']\n",
    "non_disaster_tweets.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the 'keyword' column\n",
    "The keyword column denotes a keyword from the tweet.Let's look at the top 20 keywords in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff11c9115d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],\n",
    "            orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    102\n",
       "0     40\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the 'location' column\n",
    "Even though the column `location` has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff11c7cdfd0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replacing the ambigious locations name with Standard names\n",
    "train['location'].replace({'United States':'USA',\n",
    "                           'New York':'USA',\n",
    "                            \"London\":'UK',\n",
    "                            \"Los Angeles, CA\":'USA',\n",
    "                            \"Washington, D.C.\":'USA',\n",
    "                            \"California\":'USA',\n",
    "                             \"Chicago, IL\":'USA',\n",
    "                             \"Chicago\":'USA',\n",
    "                            \"New York, NY\":'USA',\n",
    "                            \"California, USA\":'USA',\n",
    "                            \"FLorida\":'USA',\n",
    "                            \"Nigeria\":'Africa',\n",
    "                            \"Kenya\":'Africa',\n",
    "                            \"Everywhere\":'Worldwide',\n",
    "                            \"San Francisco\":'USA',\n",
    "                            \"Florida\":'USA',\n",
    "                            \"United Kingdom\":'UK',\n",
    "                            \"Los Angeles\":'USA',\n",
    "                            \"Toronto\":'Canada',\n",
    "                            \"San Francisco, CA\":'USA',\n",
    "                            \"NYC\":'USA',\n",
    "                            \"Seattle\":'USA',\n",
    "                            \"Earth\":'Worldwide',\n",
    "                            \"Ireland\":'UK',\n",
    "                            \"London, England\":'UK',\n",
    "                            \"New York City\":'USA',\n",
    "                            \"Texas\":'USA',\n",
    "                            \"London, UK\":'UK',\n",
    "                            \"Atlanta, GA\":'USA',\n",
    "                            \"Mumbai\":\"India\"},inplace=True)\n",
    "\n",
    "sns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n",
    "            orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"processing\"></a> 4. Text Data Preprocessing\n",
    "\n",
    "## 1. Data Cleaning\n",
    "\n",
    "Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the  basic text pre-processing techniques includes:\n",
    "\n",
    "* Make text all **lower case** or **uppercase** so that the algorithm does not treat the same words in different cases as different\n",
    "* **Removing Noise** i.e everything that isn’t in a standard number or letter i.e Punctuation, Numerical values,  common non-sensical text (/n)\n",
    "* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "* **Stopword Removal**: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n",
    "\n",
    "### More data cleaning steps after tokenization:\n",
    "\n",
    "* **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n",
    "* **Lemmatization**: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "And more...\n",
    "\n",
    "However, it is not necessary that you would need to use all these steps. The usage depends on your problem at hand. Sometimes removal of stop words helps while at other times, this might not help.Here is a nice table taken from the blog titled : [All you need to know about Text Preprocessing for Machine Learning & NLP](https://kavita-ganesan.com/text-preprocessing-tutorial/#.Xi2BhhczZTY) that summarizes how much preprocessing you should be performing on your text data:\n",
    "\n",
    "![](https://kavita-ganesan.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-23-at-1.36.52-PM-590x270.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Our Deeds are the Reason of this #earthquake M...\n",
       "1               Forest fire near La Ronge Sask. Canada\n",
       "2    All residents asked to 'shelter in place' are ...\n",
       "3    13,000 people receive #wildfires evacuation or...\n",
       "4    Just got sent this photo from Ruby #Alaska as ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick glance over the existing data\n",
    "train['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a first round of text cleaning techniques\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun let's create a wordcloud of the clean text to see the most dominating words in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Text:  Are you coming , aren't you\n",
      "------------------------------------------------------------------------------------------------\n",
      "Tokenization by whitespace:-  ['Are', 'you', 'coming', ',', \"aren't\", 'you']\n",
      "Tokenization by words using Treebank Word Tokenizer:-  ['Are', 'you', 'coming', ',', 'are', \"n't\", 'you']\n",
      "Tokenization by punctuation:-  ['Are', 'you', 'coming', ',', 'aren', \"'\", 't', 'you']\n",
      "Tokenization by regular expression:-  ['Are', 'you', 'coming', 'aren', 't', 'you']\n"
     ]
    }
   ],
   "source": [
    "text = \"Are you coming , aren't you\"\n",
    "tokenizer1 = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer2 = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer3 = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "print(\"Example Text: \",text)\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\n",
    "print(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\n",
    "print(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\n",
    "print(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the training and the test set\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stopwords Removal\n",
    "\n",
    "Now, let's get rid of the stopwords i.e words which occur very frequently but have no possible value like **a, an, the, are **etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token normalization\n",
    "\n",
    "Token normalisation means converting different tokens to their base forms. This can be done either by:\n",
    "\n",
    "- **Stemming** :  removing and replacing suffixes to get to the root form of the word, which is called the **stem** for instance cats - cat, wolves - wolv \n",
    "- **Lemmatization** : Returns the base or dictionary form of a word, which is known as the **lemma** \n",
    "\n",
    "[*source*](https://www.coursera.org/learn/language-processing/lecture/SCd4G/text-preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming the sentence:  feet cat wolv talk\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/marci/nltk_data'\n    - '/home/marci/anaconda3/nltk_data'\n    - '/home/marci/anaconda3/share/nltk_data'\n    - '/home/marci/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/marci/nltk_data'\n    - '/home/marci/anaconda3/nltk_data'\n    - '/home/marci/anaconda3/share/nltk_data'\n    - '/home/marci/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-4c57a1b25a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Lemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lemmatizing the sentence: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-4c57a1b25a0f>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Lemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lemmatizing the sentence: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/marci/nltk_data'\n    - '/home/marci/anaconda3/nltk_data'\n    - '/home/marci/anaconda3/share/nltk_data'\n    - '/home/marci/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization examples\n",
    "text = \"feet cats wolves talked\"\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "print(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "print(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing, the text format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting it all together- A Text Preprocessing Function\n",
    "This concludes the pre-processing part. It will be prudent to convert all the steps undertaken into a function for better reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"vectorization\"></a>  5. Transforming tokens to a vector\n",
    "After the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. This can be done by a number of tecniques:\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "The bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words.\n",
    "\n",
    "Why is it is called a “bag” of words? That is because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.\n",
    "\n",
    "For example, \n",
    "\n",
    "![](https://imgur.com/jWqtRP1.png)\n",
    "\n",
    "*source:[Natural Language Processing course on coursera](https://www.coursera.org/learn/language-processing/home/welcome)*\n",
    "\n",
    "We can do this using scikit-learn's CountVectorizer, where every row will represent a different tweet and every column will represent a different word.\n",
    "\n",
    "### Bag of Words - Countvectorizer Features\n",
    "\n",
    "[Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts. It is important to note here that CountVectorizer comes with a lot of options to automatically do preprocessing, tokenization, and stop word removal.However, i did all the process manually above to just get a better understanding. Let's use a vanilla implementation of the countvectorizer without specifying any parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 21637)\n",
      "21637\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train['text'])\n",
    "test_vectors = count_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "## Keeping only non-zero elements to preserve space \n",
    "print(train_vectors[0].todense().shape)\n",
    "print(len(count_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Features\n",
    "\n",
    "A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n",
    "\n",
    "**Term Frequency: is a scoring of the frequency of the word in the current document.**\n",
    "\n",
    "```\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tfidf = tfidf.fit_transform(train['text'])\n",
    "test_tfidf = tfidf.transform(test[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-385ffcacab22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4040\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4042\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-385ffcacab22>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-385ffcacab22>\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnopunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnopunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mremove_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "# text preprocessing function\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    \n",
    "    \n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(remove_stopwords)\n",
    "    return combined_text\n",
    "\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"model\"></a> 6. Building a Text Classification model\n",
    "Now the data is ready to be fed into a classification model. Let's create a basic claasification model using commonly used classification algorithms and see how our model performs.\n",
    "\n",
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.625     , 0.56160939, 0.62369057, 0.59318996, 0.72854914])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61094527, 0.57506824, 0.61551874, 0.59942912, 0.72245236])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf_tfidf = LogisticRegression(C=1.0)\n",
    "scores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the countvectorizer gives a better performance than TFIDF in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naives Bayes Classifier\n",
    "Well, this is a decent score. Let's try with another model that is said to work well with text data : Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6507019 , 0.62611276, 0.69247626, 0.65555556, 0.74369748])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf_NB = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB.fit(train_vectors, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59587021, 0.59205776, 0.63028169, 0.6155303 , 0.74918567])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf_NB_TFIDF = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well the naive bayes on TFIDF features scores much better than logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,model,test_vectors):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = \"../input/sample_submission.csv\"\n",
    "test_vectors=test_tfidf\n",
    "submission(submission_file_path,clf_NB_TFIDF,test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (7613, 5)\n",
      "Testing data shape:  (3263, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/2\n",
      "6851/6851 [==============================] - 6s 898us/step - loss: 0.5397 - acc: 0.7476 - val_loss: 0.4729 - val_acc: 0.7717\n",
      "Epoch 2/2\n",
      "6851/6851 [==============================] - 4s 592us/step - loss: 0.4022 - acc: 0.8250 - val_loss: 0.4676 - val_acc: 0.7638\n",
      "7613/7613 [==============================] - 1s 95us/step\n",
      "Test accuracy: 0.8577433337163756\n",
      "3263/3263 [==============================] - 1s 250us/step\n",
      "[1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "print('Testing data shape: ', test.shape)\n",
    "\n",
    "# Replacing the ambigious locations name with Standard names\n",
    "train['location'].replace({'United States':'USA',\n",
    "                           'New York':'USA',\n",
    "                            \"London\":'UK',\n",
    "                            \"Los Angeles, CA\":'USA',\n",
    "                            \"Washington, D.C.\":'USA',\n",
    "                            \"California\":'USA',\n",
    "                             \"Chicago, IL\":'USA',\n",
    "                             \"Chicago\":'USA',\n",
    "                            \"New York, NY\":'USA',\n",
    "                            \"California, USA\":'USA',\n",
    "                            \"FLorida\":'USA',\n",
    "                            \"Nigeria\":'Africa',\n",
    "                            \"Kenya\":'Africa',\n",
    "                            \"Everywhere\":'Worldwide',\n",
    "                            \"San Francisco\":'USA',\n",
    "                            \"Florida\":'USA',\n",
    "                            \"United Kingdom\":'UK',\n",
    "                            \"Los Angeles\":'USA',\n",
    "                            \"Toronto\":'Canada',\n",
    "                            \"San Francisco, CA\":'USA',\n",
    "                            \"NYC\":'USA',\n",
    "                            \"Seattle\":'USA',\n",
    "                            \"Earth\":'Worldwide',\n",
    "                            \"Ireland\":'UK',\n",
    "                            \"London, England\":'UK',\n",
    "                            \"New York City\":'USA',\n",
    "                            \"Texas\":'USA',\n",
    "                            \"London, UK\":'UK',\n",
    "                            \"Atlanta, GA\":'USA',\n",
    "                            \"Mumbai\":\"India\"},inplace=True)\n",
    "\n",
    "# Applying a first round of text cleaning techniques\n",
    "import re, string\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    \n",
    "    \n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(remove_stopwords)\n",
    "    return combined_text\n",
    "\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
    "test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
    "\n",
    "max_words = 1000\n",
    "train_text = train[\"text\"]\n",
    "test_text = test[\"text\"]\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_text) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_text)\n",
    "x_test = tokenize.texts_to_matrix(test_text)\n",
    "\n",
    "y_train = train[\"target\"]\n",
    "\n",
    "num_classes = 2\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "print(y_train[:15])\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "score = model.evaluate(x_train, y_train,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])\n",
    "result = model.predict(x_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "solution = [0 if row[0] > row[1] else 1 for row in result]\n",
    "print(solution)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,solution):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = solution\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "submission_file_path = \"../input/sample_submission.csv\"\n",
    "\n",
    "submission(submission_file_path, solution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
