{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NB_XGBoost_Logreg_tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXyRvF4-2m8",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2PBn_RV-2m-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import itertools\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# sklearn \n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
        "\n",
        "# from keras.preprocessing import text, sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USdywQuv-2nD",
        "colab_type": "text"
      },
      "source": [
        "## Define text preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-3UiRjY-2nE",
        "colab_type": "code",
        "outputId": "32d82859-13c6-4232-f511-7664a3453338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Applying a first round of text cleaning techniques\n",
        "import re, string\n",
        "from bs4 import BeautifulSoup\n",
        "tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer() \n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, 'lxml').get_text()\n",
        "    eyes = \"[8:=;]\"\n",
        "    nose = \"['`\\-]?\"\n",
        "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\" \", text)    \n",
        "    \n",
        "    text = re.sub(\"/\",\" / \", text)\n",
        "    text = re.sub('@(\\w+)', '', text)\n",
        "    \n",
        "    text = re.sub('#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}', \"<smile>\", text)\n",
        "    text = re.sub('#{eyes}#{nose}p+', \"<lolface>\", text)\n",
        "    text = re.sub('#{eyes}#{nose}\\(+|\\)+#{nose}#{eyes}', \"<sadface>\", text)\n",
        "    text = re.sub('#{eyes}#{nose}[\\/|l*]', \"<neutralface>\", text)\n",
        "    text = re.sub('<3',\"<heart>\", text)\n",
        "    # numbers\n",
        "    text = re.sub('[-+]?[.\\d]*[\\d]+[:,.\\d]*', \" \", text)\n",
        "    \n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    #text = re.sub('\\[.*?\\]', '', text)\n",
        "    #text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation.replace('<', '').replace('>', '')), ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    \n",
        "    #text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n",
        "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
        "    # Single character removal\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "    \n",
        "    #text = re.sub('\\w*\\d\\w*', '', text)    \n",
        "    \n",
        "    return text\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def text_preprocessing(text):\n",
        "  \n",
        "    nopunc = clean_text(text)\n",
        "    \n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "    \n",
        "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
        "    \n",
        "    lemmatized = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(remove_stopwords)]\n",
        "    \n",
        "    combined_text = ' '.join(lemmatized)\n",
        "    return combined_text"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4v_I0da-2nJ",
        "colab_type": "text"
      },
      "source": [
        "## Read data or preprocessed data if it exists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobsKyjd-2nQ",
        "colab_type": "code",
        "outputId": "e3523c2b-9387-47f1-f8d0-023a0991c49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train = pd.read_csv('drive/My Drive/NLP_data/train.csv')\n",
        "print('Training data shape: ', train.shape)\n",
        "test = pd.read_csv('drive/My Drive/NLP_data/test.csv')\n",
        "print('Testing data shape: ', test.shape)\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x: text_preprocessing(x))\n",
        "test['text'] = test['text'].apply(lambda x: text_preprocessing(x))\n",
        "\n",
        "train.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
        "test.drop([\"keyword\", \"location\"], axis = 1, inplace=True)\n",
        "\n",
        "train.to_csv('preprocessed_train.csv')\n",
        "test.to_csv('preprocessed_test.csv')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (7613, 5)\n",
            "Testing data shape:  (3263, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qadw8srT-2nU",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7QUB_Ud-2nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
        "train_tfidf = tfidf.fit_transform(train['text'])\n",
        "test_tfidf = tfidf.transform(test[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t37iolC5w_rT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b4589b41-6df7-4a70-e87a-06a2783ec17e"
      },
      "source": [
        "train_tfidf"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x10449 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 76809 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8poqsKBZ-2nX",
        "colab_type": "text"
      },
      "source": [
        "### Define Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOi3K1QG-2nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT1kpTdJ-2nc",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVEma3se-2nd",
        "colab_type": "code",
        "outputId": "2dd64929-2a1a-488a-ecdb-e0b989f1c609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fitting a simple Naive Bayes on TFIDF\n",
        "clf_NB_TFIDF = MultinomialNB()\n",
        "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.57473481, 0.59176672, 0.62847515, 0.61439114, 0.74363057])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLn6wc-k-2nj",
        "colab_type": "code",
        "outputId": "2a5de51c-434a-4eee-f7d2-da22c9b456e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ7BhExQNQWs",
        "colab_type": "text"
      },
      "source": [
        "Tune NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNNT-RyEMc7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXtIq3nDMOL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', MultinomialNB()),\n",
        " ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFxGwex3LoYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "556a21c0-3b1f-42aa-8e43-04b356ba74d1"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': (True, False),\n",
        "               'clf__alpha': (1e-2, 1e-3)}\n",
        "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(train[\"text\"], train[\"target\"])\n",
        "gs_clf.best_score_, gs_clf.best_params_"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6888253956202012,\n",
              " {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0KFeeLDNob-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d619d500-d93a-44ab-a3f2-e3c3b9fdee09"
      },
      "source": [
        "text_clf.set_params(clf__alpha=0.01, tfidf__use_idf=False, vect__ngram_range=(1, 1))\n",
        "scores = model_selection.cross_val_score(text_clf, train['text'], train[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.63237774, 0.61659514, 0.64705882, 0.62538226, 0.71080139])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qABqDGAJQFJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "7630e0ac-60f7-495c-8f31-9077aabf1c6c"
      },
      "source": [
        "text_clf.fit(train['text'], train[\"target\"])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=False)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfS54JgS-2np",
        "colab_type": "text"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZygo9C-2nq",
        "colab_type": "code",
        "outputId": "327d484a-c0b5-4cc2-a509-13813c4206d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "clf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "scores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.507431  , 0.39924314, 0.47359736, 0.38115632, 0.59536542])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCAUroa9-2nt",
        "colab_type": "code",
        "outputId": "fd090109-b09c-4079-cd47-121d57bd6321",
        "colab": {}
      },
      "source": [
        "clf_xgb_TFIDF.fit(train_tfidf, train[\"target\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
              "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
              "              nthread=10, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=0.8, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpfHKIfm-2nx",
        "colab_type": "code",
        "outputId": "7139044d-6a58-48e5-8e10-ccc2be27961b",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf_gb_TFIDF = GradientBoostingClassifier(n_estimators=100)\n",
        "scores = model_selection.cross_val_score(clf_gb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44540541, 0.34349593, 0.43854996, 0.32488479, 0.51889683])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wPQyEXN-2n0",
        "colab_type": "code",
        "outputId": "b35da6b4-dc6c-4616-c2b9-8957f9839da4",
        "colab": {}
      },
      "source": [
        "clf_gb_TFIDF.fit(train_tfidf, train[\"target\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='auto',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kHkcfbdBxrY",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v6ugul-BEGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "086a1979-40b8-4670-ba90-bd8e6054464b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV \n",
        "logregcv = LogisticRegressionCV(cv=5, max_iter=1000, random_state=42, class_weight='balanced').fit(train_tfidf, train[\"target\"])\n",
        "scores = model_selection.cross_val_score(logregcv, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.61896243, 0.59116466, 0.63057325, 0.56330275, 0.72684825])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbmkYBznB6TU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c9417ff4-1ed3-4fc3-895b-79bdebd57808"
      },
      "source": [
        "logregcv.fit(train_tfidf, train[\"target\"])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFzijCZtHNrt",
        "colab_type": "text"
      },
      "source": [
        "LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3da20680-f3c6-435c-fd47-f0b519178369",
        "id": "Tns0NWKgHMgT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs', class_weight='balanced')\n",
        "scores = model_selection.cross_val_score(logreg, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.62202643, 0.58578053, 0.62319939, 0.58562555, 0.71492537])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "60663d22-cae7-46b1-975f-3f5b21d056d6",
        "id": "Jq49L7zSHMgy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "logreg.fit(train_tfidf, train[\"target\"])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=42, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SPvPcat-2n2",
        "colab_type": "text"
      },
      "source": [
        "### Predict and Create submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVdwjD05-2n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def submission(submission_file_path,model,test_vectors):\n",
        "    sample_submission = pd.read_csv(submission_file_path)\n",
        "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
        "    sample_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEYxJwOe-2n5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_file_path = \"drive/My Drive/NLP_data/sample_submission.csv\"\n",
        "submission(submission_file_path,logreg,test_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}